import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import warnings
import os
import re
from scipy.stats import chi2_contingency, kruskal, mannwhitneyu, fisher_exact
from datetime import datetime
import io
from difflib import SequenceMatcher

warnings.filterwarnings('ignore')

# --- æ™ºæ…§æ’åºå‡½å¼ ---
def smart_sort_categories(categories):
    """
    æ™ºæ…§æ’åºé¡åˆ¥è³‡æ–™ï¼Œè™•ç†ï¼š
    1. ç™¾åˆ†æ¯”ç¯„åœ (å¦‚ 10-20%, 20-30%)
    2. æ•¸å€¼ç¯„åœ (å¦‚ 1-5å¹´, 5-10å¹´)
    3. é‡‘é¡ç¯„åœ (å¦‚ 100-500è¬, 500-1000è¬)
    4. éšæ®µ (ç¬¬ä¸€éšæ®µ, ç¬¬äºŒéšæ®µ, ç¬¬ä¸‰éšæ®µ)
    5. ä¸€èˆ¬æ–‡å­— (æŒ‰åŸé †åºæˆ–å­—æ¯æ’åº)
    """
    if len(categories) == 0:
        return []
    
    categories_list = list(categories)
    
    # å®šç¾©æ’åºéµå‡½å¼
    def sort_key(item):
        item_str = str(item).strip()
        
        # 1. è™•ç†ç™¾åˆ†æ¯”ç¯„åœ (å¦‚ 10-20%, 20%-30%)
        percent_match = re.match(r'(\d+\.?\d*)\s*[-~åˆ°è‡³]\s*(\d+\.?\d*)\s*[%ï¼…]', item_str)
        if percent_match:
            return (0, float(percent_match.group(1)))
        
        # å–®ä¸€ç™¾åˆ†æ¯” (å¦‚ 30%)
        single_percent = re.match(r'(\d+\.?\d*)\s*[%ï¼…]', item_str)
        if single_percent:
            return (0, float(single_percent.group(1)))
        
        # 2. è™•ç†å¹´ä»½ç¯„åœ (å¦‚ 1-5å¹´, 5-10å¹´)
        year_match = re.match(r'(\d+\.?\d*)\s*[-~åˆ°è‡³]\s*(\d+\.?\d*)\s*å¹´', item_str)
        if year_match:
            return (1, float(year_match.group(1)))
        
        # 3. è™•ç†é‡‘é¡ç¯„åœ (å¦‚ 100-500è¬, 1000-5000è¬)
        money_match = re.match(r'(\d+\.?\d*)\s*[-~åˆ°è‡³]\s*(\d+\.?\d*)\s*[è¬å„„]', item_str)
        if money_match:
            return (2, float(money_match.group(1)))
        
        # 4. è™•ç†æœˆä»½ç¯„åœ (å¦‚ 1-3å€‹æœˆ, 3-6å€‹æœˆ)
        month_match = re.match(r'(\d+\.?\d*)\s*[-~åˆ°è‡³]\s*(\d+\.?\d*)\s*å€‹?æœˆ', item_str)
        if month_match:
            return (3, float(month_match.group(1)))
        
        # 5. è™•ç†äººæ•¸ç¯„åœ (å¦‚ 1-10äºº, 10-50äºº)
        people_match = re.match(r'(\d+\.?\d*)\s*[-~åˆ°è‡³]\s*(\d+\.?\d*)\s*äºº', item_str)
        if people_match:
            return (4, float(people_match.group(1)))
        
        # 6. è™•ç†æ¬¡æ•¸ (å¦‚ æ¯æœˆ1æ¬¡, æ¯å­£1æ¬¡, æ¯å¹´1æ¬¡)
        freq_order = {'æ¯é€±': 1, 'æ¯æœˆ': 2, 'æ¯å­£': 3, 'æ¯åŠå¹´': 4, 'æ¯å¹´': 5, 'ä¸å®šæœŸ': 6, 'ç„¡': 7}
        for key, value in freq_order.items():
            if key in item_str:
                return (5, value)
        
        # 7. è™•ç†éšæ®µ (ç¬¬ä¸€éšæ®µ, ç¬¬äºŒéšæ®µ, ç¬¬ä¸‰éšæ®µ)
        stage_match = re.search(r'[ç¬¬]?([ä¸€äºŒä¸‰å››äº”1234])[éšæ®µæœŸ]', item_str)
        if stage_match:
            stage_num = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, '1': 1, '2': 2, '3': 3, '4': 4}.get(stage_match.group(1), 0)
            return (6, stage_num)
        
        # 8. è™•ç†ç¨‹åº¦ (å®Œå…¨æ²’æœ‰, éƒ¨åˆ†æœ‰, å®Œå…¨æœ‰)
        degree_order = {
            'å®Œå…¨æ²’æœ‰': 1, 'æ²’æœ‰': 1, 'ç„¡': 1,
            'æ¥µå°‘': 2, 'å¾ˆå°‘': 2,
            'éƒ¨åˆ†': 3, 'éƒ¨åˆ†æœ‰': 3, 'éƒ¨ä»½': 3,
            'å¤§éƒ¨åˆ†': 4, 'å¤§éƒ¨åˆ†æœ‰': 4,
            'å®Œå…¨': 5, 'å®Œå…¨æœ‰': 5, 'æœ‰': 5, 'æ˜¯': 5
        }
        for key, value in degree_order.items():
            if key in item_str:
                return (7, value)
        
        # 9. è™•ç†æ¯”è¼ƒç´š (ä½æ–¼, ç¬¦åˆ, é«˜æ–¼)
        compare_order = {'ä½æ–¼': 1, 'ä½': 1, 'ç¬¦åˆ': 2, 'ç›¸ç•¶': 2, 'é«˜æ–¼': 3, 'é«˜': 3, 'è¶…é': 3}
        for key, value in compare_order.items():
            if key in item_str:
                return (8, value)
        
        # 10. è™•ç†ç´”æ•¸å­—é–‹é ­
        num_match = re.match(r'^(\d+\.?\d*)', item_str)
        if num_match:
            return (9, float(num_match.group(1)))
        
        # 11. ç‰¹æ®Šè™•ç†ï¼šã€Œä»¥ä¸Šã€æ‡‰è©²æ’åœ¨æœ€å¾Œ
        if 'ä»¥ä¸Š' in item_str or 'æˆ–ä»¥ä¸Š' in item_str or 'ä»¥ä¸Š' in item_str:
            # æå–æ•¸å­—
            num_in_above = re.search(r'(\d+\.?\d*)', item_str)
            if num_in_above:
                return (10, float(num_in_above.group(1)))
        
        # 12. é è¨­ï¼šæŒ‰å­—å…¸é †åº
        return (99, item_str)
    
    # åŸ·è¡Œæ’åº
    try:
        sorted_categories = sorted(categories_list, key=sort_key)
        return sorted_categories
    except:
        # å¦‚æœæ’åºå¤±æ•—ï¼Œè¿”å›åŸé †åº
        return categories_list

# --- çµ±è¨ˆå‡½å¼å®šç¾© ---
def format_p_value(p):
    """é¡¯è‘—æ€§æ¨™è¨˜"""
    if p < 0.001:
        return f"**p={p:.4f} â­â­â­ (æ¥µé¡¯è‘—)**"
    elif p < 0.01:
        return f"**p={p:.4f} â­â­ (éå¸¸é¡¯è‘—)**"
    elif p < 0.05:
        return f"**p={p:.4f} â­ (é¡¯è‘—)**"
    else:
        return f"p={p:.4f} (ä¸é¡¯è‘—)"

def interpret_effect_size(cramers_v=None, cohens_d=None):
    """æ•ˆæœé‡è§£é‡‹"""
    if cramers_v is not None:
        if cramers_v < 0.1:
            return "æ•ˆæœé‡æ¥µå° (negligible)"
        elif cramers_v < 0.3:
            return "æ•ˆæœé‡å° (small)"
        elif cramers_v < 0.5:
            return "æ•ˆæœé‡ä¸­ç­‰ (medium)"
        else:
            return "æ•ˆæœé‡å¤§ (large)"
    elif cohens_d is not None:
        if abs(cohens_d) < 0.2:
            return "æ•ˆæœé‡æ¥µå° (negligible)"
        elif abs(cohens_d) < 0.5:
            return "æ•ˆæœé‡å° (small)"
        elif abs(cohens_d) < 0.8:
            return "æ•ˆæœé‡ä¸­ç­‰ (medium)"
        else:
            return "æ•ˆæœé‡å¤§ (large)"
    return ""

def generate_academic_conclusion(test_type, p_value, effect_size=None, groups_info=None, question_name=""):
    """ç”Ÿæˆå­¸è¡“é¢¨æ ¼çµè«–"""
    conclusion = f"\n**ğŸ“Š å­¸è¡“åˆ†æçµè«– - {question_name}**\n\n"
    
    if test_type == "chi_square":
        conclusion += f"**ç ”ç©¶æ–¹æ³•ï¼š** æ¡ç”¨å¡æ–¹æª¢å®š (Chi-square test) æª¢é©—é¡åˆ¥è®Šé …é–“çš„é—œè¯æ€§ã€‚\n\n"
        if p_value < 0.05:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°é”åˆ°é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ï¼Œ"
            if effect_size:
                conclusion += f"CramÃ©r's V = {effect_size:.3f} ({interpret_effect_size(cramers_v=effect_size)})ã€‚"
            conclusion += f"\n\n**å¯¦å‹™æ„æ¶µï¼š** å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹åœ¨æ­¤è­°é¡Œä¸Šå­˜åœ¨é¡¯è‘—å·®ç•°ï¼Œå»ºè­°é€²ä¸€æ­¥æ¢è¨å·®ç•°ä¾†æºã€‚"
        else:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°æœªé”é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ã€‚\n\n"
            conclusion += f"**å¯¦å‹™æ„æ¶µï¼š** å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹åœ¨æ­¤è­°é¡Œä¸Šçœ‹æ³•è¶¨æ–¼ä¸€è‡´ã€‚"
    
    elif test_type == "mann_whitney":
        conclusion += f"**ç ”ç©¶æ–¹æ³•ï¼š** æ¡ç”¨ Mann-Whitney U æª¢å®šï¼ˆç„¡æ¯æ•¸æª¢å®šï¼‰æ¯”è¼ƒå…©çµ„ä¸­ä½æ•¸å·®ç•°ã€‚\n\n"
        if p_value < 0.05:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°é”åˆ°é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ï¼Œ"
            if effect_size:
                conclusion += f"Cohen's d = {effect_size:.3f} ({interpret_effect_size(cohens_d=effect_size)})ã€‚"
            conclusion += f"\n\n"
            if groups_info:
                conclusion += f"**æè¿°çµ±è¨ˆï¼š**\n"
                for group, stats in groups_info.items():
                    conclusion += f"- {group}: ä¸­ä½æ•¸={stats['median']:.2f}, å¹³å‡æ•¸={stats['mean']:.2f}, æ¨™æº–å·®={stats['std']:.2f} (n={stats['n']})\n"
            conclusion += f"\n**å¯¦å‹™æ„æ¶µï¼š** å…©çµ„åœ¨æ­¤è­°é¡Œä¸Šå­˜åœ¨é¡¯è‘—å·®ç•°ï¼Œå»ºè­°é‡å°å·®ç•°ä¾†æºé€²è¡Œæ·±å…¥æ¢è¨ã€‚"
        else:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°æœªé”é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ã€‚\n\n"
            conclusion += f"**å¯¦å‹™æ„æ¶µï¼š** å…©çµ„åœ¨æ­¤è­°é¡Œä¸Šçš„çœ‹æ³•ç›¸å°ä¸€è‡´ã€‚"
    
    elif test_type == "kruskal":
        conclusion += f"**ç ”ç©¶æ–¹æ³•ï¼š** æ¡ç”¨ Kruskal-Wallis H æª¢å®šï¼ˆç„¡æ¯æ•¸æª¢å®šï¼‰æ¯”è¼ƒå¤šçµ„ä¸­ä½æ•¸å·®ç•°ã€‚\n\n"
        if p_value < 0.05:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°é”åˆ°é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ã€‚\n\n"
            if groups_info:
                conclusion += f"**æè¿°çµ±è¨ˆï¼š**\n"
                for group, stats in groups_info.items():
                    conclusion += f"- {group}: ä¸­ä½æ•¸={stats['median']:.2f}, å¹³å‡æ•¸={stats['mean']:.2f}, æ¨™æº–å·®={stats['std']:.2f} (n={stats['n']})\n"
            conclusion += f"\n**å¯¦å‹™æ„æ¶µï¼š** ä¸åŒç¾¤é«”åœ¨æ­¤è­°é¡Œä¸Šçš„èªçŸ¥æˆ–æ…‹åº¦å­˜åœ¨é¡¯è‘—å·®ç•°ï¼Œå»ºè­°é‡å°å·®ç•°è¼ƒå¤§çš„ç¾¤é«”è¨­è¨ˆå·®ç•°åŒ–ç­–ç•¥ã€‚"
        else:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°æœªé”é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ã€‚\n\n"
            conclusion += f"**å¯¦å‹™æ„æ¶µï¼š** å„ç¾¤é«”åœ¨æ­¤è­°é¡Œä¸Šçš„çœ‹æ³•ç›¸å°ä¸€è‡´ã€‚"
    
    elif test_type == "fisher":
        conclusion += f"**ç ”ç©¶æ–¹æ³•ï¼š** æ¡ç”¨ Fisher's Exact Testï¼ˆé©ç”¨æ–¼å°æ¨£æœ¬ï¼‰æª¢é©—é¡åˆ¥è®Šé …é—œè¯æ€§ã€‚\n\n"
        if p_value < 0.05:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°é”åˆ°é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ã€‚\n\n"
            conclusion += f"**å¯¦å‹™æ„æ¶µï¼š** å„˜ç®¡æ¨£æœ¬æ•¸è¼ƒå°‘ï¼Œä½†ä»è§€å¯Ÿåˆ°é¡¯è‘—å·®ç•°ï¼Œå»ºè­°æ“´å¤§æ¨£æœ¬é€²ä¸€æ­¥é©—è­‰ã€‚"
        else:
            conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** çµ±è¨ˆçµæœé¡¯ç¤ºçµ„é–“å·®ç•°æœªé”é¡¯è‘—æ°´æº– ({format_p_value(p_value)})ã€‚"
    
    elif test_type == "multiselect_chi":
        conclusion += f"**ç ”ç©¶æ–¹æ³•ï¼š** æ¡ç”¨ Presence/Absence å¡æ–¹æª¢å®šåˆ†æè¤‡é¸é¡Œå„é¸é …çš„çµ„é–“å·®ç•°ã€‚\n\n"
        conclusion += f"**ç ”ç©¶ç™¼ç¾ï¼š** è«‹åƒè€ƒä¸‹æ–¹å„é¸é …çš„çµ±è¨ˆæª¢å®šçµæœã€‚é¡¯è‘—é¸é …ä»£è¡¨è©²é¢å‘åœ¨ä¸åŒç¾¤é«”é–“æœ‰æ˜é¡¯å·®ç•°ã€‚\n\n"
        conclusion += f"**å¯¦å‹™æ„æ¶µï¼š** å»ºè­°é‡å°é¡¯è‘—å·®ç•°çš„é¸é …ï¼Œæ·±å…¥æ¢è¨å…¶èƒŒå¾ŒåŸå› ï¼Œä¸¦è€ƒæ…®èª¿æ•´ç›¸æ‡‰æ”¿ç­–æˆ–æºé€šç­–ç•¥ã€‚"
    
    return conclusion

def _cramers_v_from_table(table):
    try:
        chi2, p, dof, exp = chi2_contingency(table)
        if np.nanmin(exp) <= 1:
            return None, None, exp
        n = table.values.sum()
        return np.sqrt(chi2 / (n * (min(table.shape) - 1))), p, exp
    except Exception:
        return None, None, None

def compute_and_display_categorical_stats(df, series):
    if PHASE_COLUMN_NAME in df.columns and df[PHASE_COLUMN_NAME].notna().any() and df[PHASE_COLUMN_NAME].nunique() > 1:
        phases = df[PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»éšæ®µ')
        table = pd.crosstab(series.astype(str), phases)
        st.markdown("**è·¨éšæ®µçµ±è¨ˆï¼ˆé¡åˆ¥ï¼‰**")
        st.dataframe(table)
        cramers, p, exp = _cramers_v_from_table(table)
        if exp is None:
            st.write("ç„¡æ³•è¨ˆç®—å¡æ–¹æª¢å®šï¼ˆç™¼ç”ŸéŒ¯èª¤ï¼‰ã€‚")
        else:
            if np.nanmin(exp) <= 1:
                st.write("å¡æ–¹æª¢å®šæœªåŸ·è¡Œï¼šæŸäº› cell çš„æœŸæœ›æ¬¡æ•¸ â‰¤ 1ã€‚å»ºè­°åˆä½µé¡åˆ¥æˆ–æ”¹ç”¨å…¶ä»–æª¢å®šæ–¹æ³•ã€‚")
                st.write("æœŸæœ›æ¬¡æ•¸çŸ©é™£ï¼š")
                st.dataframe(pd.DataFrame(exp, index=table.index, columns=table.columns))
            else:
                if p is not None:
                    st.write(f"å¡æ–¹æª¢å®š {format_p_value(p)}ï¼›Cramer's V = {cramers:.3f} ({interpret_effect_size(cramers_v=cramers)})")
                else:
                    st.write("ç„¡æ³•è¨ˆç®—å¡æ–¹æª¢å®šçµæœã€‚")
    else:
        st.write("æœªåŒ…å«å¤šå€‹éšæ®µï¼Œæœªé€²è¡Œè·¨éšæ®µé¡åˆ¥æª¢å®šã€‚")

def compute_and_display_numeric_stats(df, series):
    if PHASE_COLUMN_NAME in df.columns and df[PHASE_COLUMN_NAME].notna().any() and df[PHASE_COLUMN_NAME].nunique() > 1:
        phases = df[PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»éšæ®µ')
        groups = []
        labels = []
        for ph in phases.unique():
            grp = pd.to_numeric(series[phases == ph].dropna(), errors='coerce').dropna().astype(float)
            if len(grp) > 0:
                groups.append(grp)
                labels.append(ph)
        st.markdown("**è·¨éšæ®µçµ±è¨ˆï¼ˆæ•¸å€¼ï¼‰**")
        summaries = {lab: f"n={len(g)}, mean={g.mean():.3f}, median={g.median():.3f}, std={g.std(ddof=0):.3f}" for lab, g in zip(labels, groups)}
        st.write(summaries)
        if len(groups) > 1:
            try:
                all_vals = np.concatenate([g.values for g in groups]) if groups else np.array([])
                if all_vals.size > 0 and np.all(all_vals == all_vals[0]):
                    st.write("æ‰€æœ‰çµ„åˆ¥çš„æ•¸å€¼å®Œå…¨ç›¸åŒï¼ŒKruskal-Wallis æª¢å®šä¸é©ç”¨ã€‚")
                else:
                    stat, p = kruskal(*groups)
                    st.write(f"Kruskal-Wallis stat={stat:.3f}, {format_p_value(p)}")
            except ValueError as e:
                st.write("Kruskal-Wallis æª¢å®šéŒ¯èª¤ï¼š", e)
            except Exception as e:
                st.write("åŸ·è¡Œ Kruskal-Wallis æª¢å®šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š", e)
        else:
            st.write("æ¯å€‹éšæ®µæ¨£æœ¬ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œ Kruskal-Wallis æª¢å®šã€‚")
    else:
        st.write("æœªåŒ…å«å¤šå€‹éšæ®µï¼Œæœªé€²è¡Œè·¨éšæ®µæ•¸å€¼æª¢å®šã€‚")

def compute_and_display_multiselect_option_tests(df, original_series, option_list):
    if PHASE_COLUMN_NAME in df.columns and df[PHASE_COLUMN_NAME].notna().any() and df[PHASE_COLUMN_NAME].nunique() > 1:
        st.markdown("**è¤‡é¸é¡Œé¸é …è·¨éšæ®µçµ±è¨ˆï¼ˆPresence/Absence å¡æ–¹ï¼‰**")
        phases = df[PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»éšæ®µ')
        for opt in option_list:
            pres = original_series.astype(str).fillna('').apply(lambda s: opt in [x.strip() for x in s.split('\n') if x.strip()!=''])
            table = pd.crosstab(pres, phases)
            if table.size == 0 or table.values.sum() == 0 or table.shape[0] < 2:
                st.write(f"é¸é … '{opt}'ï¼šæ¨£æœ¬æˆ–åˆ†é¡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œå¡æ–¹æª¢å®šã€‚")
                continue
            try:
                chi2, p, dof, exp = chi2_contingency(table)
                if np.nanmin(exp) <= 1:
                    st.write(f"é¸é … '{opt}'ï¼šæœŸæœ›æ¬¡æ•¸éå° (â‰¤1)ï¼Œè·³éæª¢å®šã€‚")
                else:
                    n = table.values.sum()
                    cramers = np.sqrt(chi2 / (n * (min(table.shape) - 1))) if n and min(table.shape) > 1 else None
                    st.write(f"é¸é … '{opt}'ï¼š{format_p_value(p)}" + (f"ï¼›Cramer's V={cramers:.3f} ({interpret_effect_size(cramers_v=cramers)})" if cramers is not None else ""))
            except Exception as e:
                st.write(f"é¸é … '{opt}' ç„¡æ³•è¨ˆç®—å¡æ–¹æª¢å®šï¼š{e}")
    else:
        st.write("æœªåŒ…å«å¤šå€‹éšæ®µï¼Œæœªé€²è¡Œè¤‡é¸é¡Œè·¨éšæ®µæª¢å®šã€‚")

def perform_comprehensive_statistical_analysis(df, col_data, col_name, is_numeric=False, is_multiselect=False):
    """
    ç¶œåˆçµ±è¨ˆåˆ†æï¼šåˆ†æå…¬å¸æ–¹ vs æŠ•è³‡æ–¹ã€ä¸åŒéšæ®µä¹‹é–“çš„å·®ç•°
    """
    st.markdown("---")
    st.markdown("### ğŸ“ˆ çµ±è¨ˆåˆ†æå ±å‘Š")
    
    has_respondent_type = 'respondent_type' in df.columns and df['respondent_type'].notna().any()
    has_phase = PHASE_COLUMN_NAME in df.columns and df[PHASE_COLUMN_NAME].notna().any()
    
    if not has_respondent_type and not has_phase:
        st.info("è³‡æ–™ä¸­ç„¡èº«åˆ†æˆ–éšæ®µè³‡è¨Šï¼Œç„¡æ³•é€²è¡Œåˆ†çµ„çµ±è¨ˆåˆ†æã€‚")
        return
    
    # 1. å…¬å¸æ–¹ vs æŠ•è³‡æ–¹åˆ†æ
    if has_respondent_type:
        st.markdown("#### ğŸ¢ å…¬å¸æ–¹ vs æŠ•è³‡æ–¹æ¯”è¼ƒåˆ†æ")
        
        respondent_data = df.loc[col_data.index, 'respondent_type']
        valid_types = respondent_data[respondent_data.isin(['å…¬å¸æ–¹', 'æŠ•è³‡æ–¹'])]
        
        if len(valid_types.unique()) >= 2:
            if is_numeric:
                # æ•¸å€¼å‹è³‡æ–™ï¼šMann-Whitney U æª¢å®š
                company_vals = pd.to_numeric(col_data[respondent_data == 'å…¬å¸æ–¹'], errors='coerce').dropna()
                investor_vals = pd.to_numeric(col_data[respondent_data == 'æŠ•è³‡æ–¹'], errors='coerce').dropna()
                
                if len(company_vals) > 0 and len(investor_vals) > 0:
                    st.markdown("**æè¿°çµ±è¨ˆï¼š**")
                    stats_df = pd.DataFrame({
                        'ç¾¤é«”': ['å…¬å¸æ–¹', 'æŠ•è³‡æ–¹'],
                        'æ¨£æœ¬æ•¸': [len(company_vals), len(investor_vals)],
                        'å¹³å‡æ•¸': [company_vals.mean(), investor_vals.mean()],
                        'ä¸­ä½æ•¸': [company_vals.median(), investor_vals.median()],
                        'æ¨™æº–å·®': [company_vals.std(), investor_vals.std()],
                        'æœ€å°å€¼': [company_vals.min(), investor_vals.min()],
                        'æœ€å¤§å€¼': [company_vals.max(), investor_vals.max()]
                    })
                    st.dataframe(stats_df.style.format({
                        'å¹³å‡æ•¸': '{:.2f}', 'ä¸­ä½æ•¸': '{:.2f}', 'æ¨™æº–å·®': '{:.2f}',
                        'æœ€å°å€¼': '{:.2f}', 'æœ€å¤§å€¼': '{:.2f}'
                    }), use_container_width=True)
                    
                    try:
                        stat, p = mannwhitneyu(company_vals, investor_vals, alternative='two-sided')
                        st.markdown("**Mann-Whitney U æª¢å®šçµæœï¼š**")
                        st.write(f"- U çµ±è¨ˆé‡ = {stat:.2f}")
                        st.write(f"- {format_p_value(p)}")
                        
                        # Cohen's d æ•ˆæœé‡
                        pooled_std = np.sqrt(((len(company_vals)-1)*company_vals.std()**2 + (len(investor_vals)-1)*investor_vals.std()**2) / (len(company_vals)+len(investor_vals)-2))
                        cohens_d = (company_vals.mean() - investor_vals.mean()) / pooled_std if pooled_std > 0 else 0
                        st.write(f"- Cohen's d = {cohens_d:.3f} ({interpret_effect_size(cohens_d=cohens_d)})")
                        
                        st.markdown(generate_academic_conclusion(
                            test_type="mann_whitney",
                            p_value=p,
                            effect_size=cohens_d,
                            groups_info={
                                'å…¬å¸æ–¹': {'n': len(company_vals), 'mean': company_vals.mean(), 'median': company_vals.median(), 'std': company_vals.std()},
                                'æŠ•è³‡æ–¹': {'n': len(investor_vals), 'mean': investor_vals.mean(), 'median': investor_vals.median(), 'std': investor_vals.std()}
                            },
                            question_name="å…¬å¸æ–¹ vs æŠ•è³‡æ–¹"
                        ))
                    except Exception as e:
                        st.warning(f"ç„¡æ³•åŸ·è¡Œ Mann-Whitney U æª¢å®šï¼š{e}")
                else:
                    st.info("å…¬å¸æ–¹æˆ–æŠ•è³‡æ–¹çš„æ¨£æœ¬æ•¸ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œçµ±è¨ˆæª¢å®šã€‚")
            
            elif is_multiselect:
                # è¤‡é¸é¡Œï¼šå°æ¯å€‹é¸é …é€²è¡Œå¡æ–¹æª¢å®š
                st.markdown("**è¤‡é¸é¡Œé¸é …åˆ†æï¼ˆå…¬å¸æ–¹ vs æŠ•è³‡æ–¹ï¼‰ï¼š**")
                exploded = col_data.astype(str).str.split('\n').explode().str.strip()
                exploded = exploded[(exploded != '') & (exploded != 'nan') & exploded.notna()]
                
                if not exploded.empty:
                    options = exploded.unique()
                    for opt in options[:10]:  # é™åˆ¶å‰10å€‹é¸é …é¿å…éå¤š
                        has_opt = col_data.astype(str).apply(lambda x: opt in [s.strip() for s in str(x).split('\n') if s.strip()])
                        opt_data = pd.DataFrame({
                            'has_option': has_opt[respondent_data.isin(['å…¬å¸æ–¹', 'æŠ•è³‡æ–¹'])],
                            'respondent': respondent_data[respondent_data.isin(['å…¬å¸æ–¹', 'æŠ•è³‡æ–¹'])]
                        }).dropna()
                        
                        if len(opt_data) > 0:
                            table = pd.crosstab(opt_data['has_option'], opt_data['respondent'])
                            if table.shape[0] >= 2 and table.shape[1] >= 2:
                                try:
                                    chi2, p, dof, exp = chi2_contingency(table)
                                    if np.nanmin(exp) > 1:
                                        n = table.values.sum()
                                        cramers = np.sqrt(chi2 / (n * (min(table.shape) - 1)))
                                        st.write(f"**é¸é …ã€Œ{opt}ã€ï¼š** {format_p_value(p)}ï¼ŒCramÃ©r's V = {cramers:.3f}")
                                except Exception:
                                    pass
            
            else:
                # é¡åˆ¥å‹è³‡æ–™ï¼šå¡æ–¹æª¢å®š
                category_data = col_data[respondent_data.isin(['å…¬å¸æ–¹', 'æŠ•è³‡æ–¹'])].astype(str)
                category_data = category_data[~category_data.str.lower().str.contains('nan', na=False)]
                respondent_filtered = respondent_data[category_data.index]
                
                if len(category_data) > 0:
                    table = pd.crosstab(category_data, respondent_filtered)
                    
                    st.markdown("**äº¤å‰åˆ—è¯è¡¨ï¼š**")
                    st.dataframe(table, use_container_width=True)
                    
                    if table.shape[0] >= 2 and table.shape[1] >= 2:
                        try:
                            chi2, p, dof, exp = chi2_contingency(table)
                            
                            if np.nanmin(exp) > 1:
                                n = table.values.sum()
                                cramers = np.sqrt(chi2 / (n * (min(table.shape) - 1)))
                                
                                st.markdown("**å¡æ–¹æª¢å®šçµæœï¼š**")
                                st.write(f"- Ï‡Â² = {chi2:.2f}, df = {dof}")
                                st.write(f"- {format_p_value(p)}")
                                st.write(f"- CramÃ©r's V = {cramers:.3f} ({interpret_effect_size(cramers_v=cramers)})")
                                
                                st.markdown(generate_academic_conclusion(
                                    test_type="chi_square",
                                    p_value=p,
                                    effect_size=cramers,
                                    question_name="å…¬å¸æ–¹ vs æŠ•è³‡æ–¹"
                                ))
                            else:
                                st.warning("æœŸæœ›æ¬¡æ•¸éå°ï¼ˆ<1ï¼‰ï¼Œæ”¹ç”¨ Fisher's Exact Test")
                                try:
                                    if table.shape == (2, 2):
                                        oddsratio, p = fisher_exact(table)
                                        st.write(f"- {format_p_value(p)}")
                                        st.write(f"- Odds Ratio = {oddsratio:.3f}")
                                except Exception as e:
                                    st.warning(f"ç„¡æ³•åŸ·è¡Œ Fisher's Exact Testï¼š{e}")
                        except Exception as e:
                            st.warning(f"ç„¡æ³•åŸ·è¡Œå¡æ–¹æª¢å®šï¼š{e}")
        else:
            st.info("åªæœ‰å–®ä¸€èº«åˆ†é¡å‹ï¼Œç„¡æ³•é€²è¡Œå…¬å¸æ–¹ vs æŠ•è³‡æ–¹æ¯”è¼ƒã€‚")
    
    # 2. ä¸åŒéšæ®µåˆ†æ
    if has_phase and df[PHASE_COLUMN_NAME].nunique() > 1:
        st.markdown("#### ğŸ“Š ä¸åŒéšæ®µæ¯”è¼ƒåˆ†æ")
        
        phase_data = df.loc[col_data.index, PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»éšæ®µ')
        
        if len(phase_data.unique()) >= 2:
            if is_numeric:
                # æ•¸å€¼å‹è³‡æ–™ï¼šKruskal-Wallis H æª¢å®š
                groups = []
                labels = []
                groups_info = {}
                
                for phase in sorted(phase_data.unique()):
                    phase_vals = pd.to_numeric(col_data[phase_data == phase], errors='coerce').dropna()
                    if len(phase_vals) > 0:
                        groups.append(phase_vals)
                        labels.append(phase)
                        groups_info[phase] = {
                            'n': len(phase_vals),
                            'mean': phase_vals.mean(),
                            'median': phase_vals.median(),
                            'std': phase_vals.std()
                        }
                
                if len(groups) >= 2:
                    st.markdown("**å„éšæ®µæè¿°çµ±è¨ˆï¼š**")
                    phase_stats_df = pd.DataFrame([
                        {
                            'éšæ®µ': label,
                            'æ¨£æœ¬æ•¸': info['n'],
                            'å¹³å‡æ•¸': info['mean'],
                            'ä¸­ä½æ•¸': info['median'],
                            'æ¨™æº–å·®': info['std']
                        }
                        for label, info in groups_info.items()
                    ])
                    st.dataframe(phase_stats_df.style.format({
                        'å¹³å‡æ•¸': '{:.2f}', 'ä¸­ä½æ•¸': '{:.2f}', 'æ¨™æº–å·®': '{:.2f}'
                    }), use_container_width=True)
                    
                    try:
                        stat, p = kruskal(*groups)
                        st.markdown("**Kruskal-Wallis H æª¢å®šçµæœï¼š**")
                        st.write(f"- H çµ±è¨ˆé‡ = {stat:.2f}")
                        st.write(f"- {format_p_value(p)}")
                        
                        st.markdown(generate_academic_conclusion(
                            test_type="kruskal",
                            p_value=p,
                            groups_info=groups_info,
                            question_name="éšæ®µæ¯”è¼ƒ"
                        ))
                    except Exception as e:
                        st.warning(f"ç„¡æ³•åŸ·è¡Œ Kruskal-Wallis æª¢å®šï¼š{e}")
            
            elif is_multiselect:
                # è¤‡é¸é¡Œï¼šå°æ¯å€‹é¸é …é€²è¡Œéšæ®µé–“å¡æ–¹æª¢å®š
                st.markdown("**è¤‡é¸é¡Œé¸é …éšæ®µåˆ†æï¼š**")
                compute_and_display_multiselect_option_tests(df, col_data, 
                    col_data.astype(str).str.split('\n').explode().str.strip().unique()[:10])
            
            else:
                # é¡åˆ¥å‹è³‡æ–™ï¼šå¡æ–¹æª¢å®š
                category_data = col_data.astype(str)
                category_data = category_data[~category_data.str.lower().str.contains('nan', na=False)]
                phase_filtered = phase_data[category_data.index]
                
                if len(category_data) > 0:
                    table = pd.crosstab(category_data, phase_filtered)
                    
                    st.markdown("**éšæ®µäº¤å‰åˆ—è¯è¡¨ï¼š**")
                    st.dataframe(table, use_container_width=True)
                    
                    if table.shape[0] >= 2 and table.shape[1] >= 2:
                        try:
                            chi2, p, dof, exp = chi2_contingency(table)
                            
                            if np.nanmin(exp) > 1:
                                n = table.values.sum()
                                cramers = np.sqrt(chi2 / (n * (min(table.shape) - 1)))
                                
                                st.markdown("**å¡æ–¹æª¢å®šçµæœï¼š**")
                                st.write(f"- Ï‡Â² = {chi2:.2f}, df = {dof}")
                                st.write(f"- {format_p_value(p)}")
                                st.write(f"- CramÃ©r's V = {cramers:.3f} ({interpret_effect_size(cramers_v=cramers)})")
                                
                                st.markdown(generate_academic_conclusion(
                                    test_type="chi_square",
                                    p_value=p,
                                    effect_size=cramers,
                                    question_name="éšæ®µæ¯”è¼ƒ"
                                ))
                            else:
                                st.warning("æœŸæœ›æ¬¡æ•¸éå°ï¼ˆ<1ï¼‰ï¼Œå»ºè­°åˆä½µé¡åˆ¥æˆ–å¢åŠ æ¨£æœ¬æ•¸")
                        except Exception as e:
                            st.warning(f"ç„¡æ³•åŸ·è¡Œå¡æ–¹æª¢å®šï¼š{e}")

st.set_page_config(layout="wide", page_title="å•å·äº’å‹•åˆ†æå ±å‘Š")

@st.cache_data
def load_and_concat(file_paths):
    all_dfs = []
    for path in file_paths:
        if not isinstance(path, str) or path.strip() == "":
            continue
        if not os.path.exists(path):
            continue
        df = None
        for enc in ("utf-8", "utf-8-sig", "latin1"):
            try:
                df = pd.read_csv(path, encoding=enc)
                break
            except Exception:
                pass
        if df is None:
            continue
        try:
            df.columns = df.columns.str.replace(r'ã€.*?ã€‘', '', regex=True).str.strip()
            df.columns = df.columns.str.replace('\n', ' ', regex=False)
        except Exception:
            pass
        try:
            if PHASE_COLUMN_NAME in df.columns:
                extracted = df[PHASE_COLUMN_NAME].astype(str).str.extract(r'(ç¬¬ä¸€éšæ®µ|ç¬¬äºŒéšæ®µ|ç¬¬ä¸‰éšæ®µ)', expand=False)
                df[PHASE_COLUMN_NAME] = extracted.where(extracted.notna(), df[PHASE_COLUMN_NAME])
            else:
                m = re.search(r'(ç¬¬ä¸€éšæ®µ|ç¬¬äºŒéšæ®µ|ç¬¬ä¸‰éšæ®µ)', os.path.basename(path))
                if m:
                    df[PHASE_COLUMN_NAME] = m.group(1)
        except Exception:
            pass
        df['_source_file'] = os.path.basename(path)
        all_dfs.append(df)
    if not all_dfs:
        return pd.DataFrame()
    return pd.concat(all_dfs, ignore_index=True, sort=False)

st.title("ğŸ“Š å•å·è³‡æ–™äº’å‹•åˆ†æå ±å‘Š")
st.markdown("è«‹å…ˆé¸æ“‡åˆ†ææ¨¡å¼ï¼Œç„¶å¾Œå†æ ¹æ“šæç¤ºé¸æ“‡è¦æŸ¥çœ‹çš„è³‡æ–™ç¯„åœã€‚")

# --- File Definitions ---
COMPANY_P1_FILE = "STANDARD_8RG8Y_æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·ç¬¬ä¸€éšæ®µ_202511050604_690ae8db08878.csv"
COMPANY_P2_FILE = "STANDARD_7RGxP_æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·ç¬¬äºŒéšæ®µ_202511050605_690ae92a9a127.csv"
COMPANY_P3_FILE = "STANDARD_Yb9D2_æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·ç¬¬ä¸‰éšæ®µ_202511050605_690ae9445a228.csv"
INVESTOR_P1_FILE = "STANDARD_NwNYM_æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·ç¬¬ä¸€éšæ®µæŠ•è³‡æ–¹_202511060133_690bfaccec28e.csv"
INVESTOR_P2_FILE = "STANDARD_v2xYO_æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·ç¬¬äºŒéšæ®µæŠ•è³‡æ–¹_202511060133_690bfae9b9065.csv"
INVESTOR_P3_FILE = "STANDARD_we89e_æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·ç¬¬ä¸‰éšæ®µæŠ•è³‡æ–¹_202511060133_690bfb0524491.csv"
COMPANY_NEW_MULTIPHASE_FILE = "STANDARD_v2xkX_æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·_202511060532_690c3305c62b5.csv"
PHASE_COLUMN_NAME = "è«‹å•å…¬å¸ç›®å‰ä¸»è¦è™•æ–¼å“ªå€‹ç™¼å±•éšæ®µï¼Ÿï¼š"

company_files = {"ç¬¬ä¸€éšæ®µ": COMPANY_P1_FILE, "ç¬¬äºŒéšæ®µ": COMPANY_P2_FILE, "ç¬¬ä¸‰éšæ®µ": COMPANY_P3_FILE}
investor_files = {"ç¬¬ä¸€éšæ®µ": INVESTOR_P1_FILE, "ç¬¬äºŒéšæ®µ": INVESTOR_P2_FILE, "ç¬¬ä¸‰éšæ®µ": INVESTOR_P3_FILE}
ALL_FILES = list(company_files.values()) + list(investor_files.values()) + [COMPANY_NEW_MULTIPHASE_FILE]

RESP_COLOR_MAP = {
    "å…¬å¸æ–¹": "#1f77b4",
    "æŠ•è³‡æ–¹": "#ff7f0e",
    "æœªçŸ¥":   "#7f7f7f"
}

# --- UI Logic ---
analysis_mode = st.radio("**æ­¥é©Ÿä¸€ï¼šè«‹é¸æ“‡åˆ†ææ¨¡å¼**", ('é€é¡Œç€è¦½', 'åˆä½µåˆ†æ'), horizontal=True, key="main_mode")

df_to_analyze = None
report_title = ""
files_to_load = []

if analysis_mode == 'é€é¡Œç€è¦½':
    data_source = st.radio("**æ­¥é©ŸäºŒï¼šè«‹é¸æ“‡è¦åˆ†æçš„å°è±¡**", ('å…¬å¸æ–¹', 'æŠ•è³‡æ–¹'), horizontal=True, key="data_source")
    files = company_files if data_source == 'å…¬å¸æ–¹' else investor_files
    phase_options = ["ä¸åˆ†éšæ®µ (å…¨éƒ¨åˆä½µ)"] + list(files.keys())
    selected_phase = st.radio("**æ­¥é©Ÿä¸‰ï¼šè«‹é¸æ“‡å•å·éšæ®µ**", phase_options, horizontal=False, key="phase_select")
    
    if selected_phase == "ä¸åˆ†éšæ®µ (å…¨éƒ¨åˆä½µ)":
        files_to_load = list(files.values())
        if data_source == 'å…¬å¸æ–¹':
            files_to_load.append(COMPANY_NEW_MULTIPHASE_FILE)
        report_title = f"{data_source} - ä¸åˆ†éšæ®µ (å…¨éƒ¨åˆä½µ)"
    else:
        files_to_load = [files[selected_phase]]
        if data_source == 'å…¬å¸æ–¹':
            files_to_load.append(COMPANY_NEW_MULTIPHASE_FILE)
        report_title = f"{data_source} - {selected_phase}"
    
    df_to_analyze = load_and_concat(files_to_load)
    
    if selected_phase != "ä¸åˆ†éšæ®µ (å…¨éƒ¨åˆä½µ)" and data_source == 'å…¬å¸æ–¹':
        if PHASE_COLUMN_NAME in df_to_analyze.columns:
            df_to_analyze = df_to_analyze[
                (df_to_analyze['_source_file'].str.contains(selected_phase.replace('éšæ®µ', ''), na=False)) |
                (df_to_analyze[PHASE_COLUMN_NAME].astype(str).str.contains(selected_phase, na=False))
            ]

elif analysis_mode == 'åˆä½µåˆ†æ':
    combine_option = st.radio("**æ­¥é©ŸäºŒï¼šè«‹é¸æ“‡åˆä½µæ–¹å¼**", ('åˆä½µæ‰€æœ‰éšæ®µ', 'åˆä½µç¬¬ä¸€éšæ®µ', 'åˆä½µç¬¬äºŒéšæ®µ', 'åˆä½µç¬¬ä¸‰éšæ®µ'), horizontal=False, key="combine_option")
    
    if combine_option == 'åˆä½µæ‰€æœ‰éšæ®µ':
        files_to_load = list(company_files.values()) + list(investor_files.values()) + [COMPANY_NEW_MULTIPHASE_FILE]
        report_title = "å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹ - æ‰€æœ‰éšæ®µåˆä½µ"
    elif combine_option == 'åˆä½µç¬¬ä¸€éšæ®µ':
        files_to_load = [COMPANY_P1_FILE, INVESTOR_P1_FILE, COMPANY_NEW_MULTIPHASE_FILE]
        report_title = "å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹ - ç¬¬ä¸€éšæ®µ"
    elif combine_option == 'åˆä½µç¬¬äºŒéšæ®µ':
        files_to_load = [COMPANY_P2_FILE, INVESTOR_P2_FILE, COMPANY_NEW_MULTIPHASE_FILE]
        report_title = "å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹ - ç¬¬äºŒéšæ®µ"
    else:
        files_to_load = [COMPANY_P3_FILE, INVESTOR_P3_FILE, COMPANY_NEW_MULTIPHASE_FILE]
        report_title = "å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹ - ç¬¬ä¸‰éšæ®µ"
    
    df_to_analyze = load_and_concat(files_to_load)
    
    if combine_option != 'åˆä½µæ‰€æœ‰éšæ®µ':
        target_phase = combine_option.replace('åˆä½µ', '')
        if PHASE_COLUMN_NAME in df_to_analyze.columns:
            df_to_analyze = df_to_analyze[
                (df_to_analyze['_source_file'].str.contains(target_phase.replace('éšæ®µ', ''), na=False)) |
                (df_to_analyze[PHASE_COLUMN_NAME].astype(str).str.contains(target_phase, na=False))
            ]

# æ¨™è¨˜å¡«ç­”è€…èº«åˆ†
if df_to_analyze is not None and not df_to_analyze.empty:
    try:
        if '_source_file' in df_to_analyze.columns:
            def infer_role(fname):
                if not isinstance(fname, str): return 'æœªçŸ¥'
                if 'æŠ•è³‡' in fname or 'INVEST' in fname.upper():
                    return 'æŠ•è³‡æ–¹'
                return 'å…¬å¸æ–¹'
            df_to_analyze['respondent_type'] = df_to_analyze['_source_file'].astype(str).apply(infer_role)
        else:
            df_to_analyze['respondent_type'] = 'æœªçŸ¥'
    except Exception:
        df_to_analyze['respondent_type'] = 'æœªçŸ¥'

if df_to_analyze is None or df_to_analyze.empty:
    st.warning("åœ¨æ­¤é¸æ“‡ä¸‹æ²’æœ‰è¼‰å…¥ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥æ‚¨çš„é¸æ“‡å’Œæª”æ¡ˆã€‚")
    st.stop()

# --- Display Analysis ---
st.header(f"æ‚¨æ­£åœ¨æŸ¥çœ‹ï¼š{report_title}çš„åˆ†æçµæœ")

col_metric1, col_metric2 = st.columns(2)
with col_metric1:
    st.metric("ç¸½æ¨£æœ¬æ•¸ (å•å·ä»½æ•¸)", len(df_to_analyze))
with col_metric2:
    if len(df_to_analyze) < 30:
        st.warning("âš ï¸ æ¨£æœ¬æ•¸ < 30ï¼Œçµ±è¨ˆæª¢å®šçµæœå¯èƒ½ä¸ç©©å®š")

cols_to_exclude = ['ç‚ºäº†å¾ŒçºŒæ”¯ä»˜è¨ªè«‡è²»ï¼Œè«‹æä¾›æ‚¨çš„é›»å­éƒµä»¶åœ°å€ï¼ˆæˆ‘å€‘å°‡åƒ…ç”¨æ–¼è¯ç¹«æ‚¨æ”¯ä»˜è¨ªè«‡è²»ï¼Œä¸¦å¦¥å–„ä¿è­·æ‚¨çš„è³‡æ–™ï¼‰:', 'IPç´€éŒ„', 'é¡æ»¿çµæŸè¨»è¨˜', 'ä½¿ç”¨è€…ç´€éŒ„', 'æœƒå“¡æ™‚é–“', 'Hash', 'æœƒå“¡ç·¨è™Ÿ', 'è‡ªè¨‚ID', 'å‚™è¨»', 'å¡«ç­”æ™‚é–“', PHASE_COLUMN_NAME, '_source_file', 'respondent_type']
# é¡Œç›®æ¨™æº–åŒ–å‡½æ•¸
def normalize_question(q):
    """æ¨™æº–åŒ–é¡Œç›®ï¼šç§»é™¤ã€Œå…¬å¸ã€ã€ã€Œæ‚¨æŠ•è³‡çš„å…¬å¸ã€ç­‰å·®ç•°"""
    if not isinstance(q, str):
        return q
    
    # ç§»é™¤ã€Œæœªå‘½åé¡Œç›® - ã€å‰ç¶´
    q = re.sub(r'^æœªå‘½åé¡Œç›®[\s\-ï¼š:]+', '', q)
    
    # ç§»é™¤å¸¸è¦‹çš„èº«åˆ†å€åˆ¥è©ï¼ˆæ›´å…¨é¢çš„è¦å‰‡ï¼‰
    q = q.replace('æ‚¨æŠ•è³‡çš„å…¬å¸æœ‰', 'å…¬å¸')
    q = q.replace('æ‚¨æŠ•è³‡çš„å…¬å¸', 'å…¬å¸')
    q = q.replace('è²´å…¬å¸æœ‰', 'å…¬å¸')
    q = q.replace('è²´å…¬å¸', 'å…¬å¸')
    q = q.replace('å…¬å¸æœ‰', 'å…¬å¸')
    q = q.replace('å…¬å¸æ˜¯å¦', 'å…¬å¸')
    q = q.replace('æ‚¨èªç‚ºå…¬å¸', 'å…¬å¸')
    q = q.replace('æ‚¨èªç‚º', '')
    
    # ç§»é™¤é¡Œç›®é–‹é ­çš„èº«åˆ†å‰ç¶´ï¼ˆåŒ…å«ç©ºæ ¼ã€ç ´æŠ˜è™Ÿã€å†’è™Ÿç­‰ï¼‰
    q = re.sub(r'^(å…¬å¸|æŠ•è³‡æ–¹|å…¬å¸æ–¹)[\s\-ï¼š:]+', '', q)
    
    # çµ±ä¸€ã€Œ-ã€ç¬¦è™Ÿï¼ˆå…¨å½¢ã€åŠå½¢ç ´æŠ˜è™Ÿï¼‰
    q = q.replace('ï¼', '-').replace('â€”', '-').replace('â€“', '-')
    
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    q = re.sub(r'\s+', ' ', q).strip()
    
    # ç§»é™¤å°¾éƒ¨çš„å†’è™Ÿæˆ–å¥è™Ÿ
    q = q.rstrip('ï¼š:ã€‚.')
    
    return q

def normalize_question_v2(q):
    """æ›´æ¿€é€²çš„æ¨™æº–åŒ–ï¼šç§»é™¤æ‰€æœ‰èº«åˆ†æ¨™è¨˜å’Œå†—é¤˜è©å½™"""
    if not isinstance(q, str):
        return q
    
    # 0. ç‰¹æ®Šè™•ç†ï¼šå…§éƒ¨æ§åˆ¶å¾ªç’°é¡Œç›®ï¼ˆå®Œå…¨çµ±ä¸€æ ¼å¼ï¼‰
    if 'å…§éƒ¨æ§åˆ¶å¾ªç’°' in q and 'å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•' in q:
        # å…ˆç§»é™¤é …ç›®åˆ—è¡¨ (1)(2)(3)...
        q = re.sub(r'\s*[\(ï¼ˆ]1[\)ï¼‰][^ï¼Ÿ?]*', '', q)
        # å†çµ±ä¸€æ–‡å­—å…§å®¹ï¼ˆç§»é™¤æ¨™é»å’Œå•è™Ÿï¼‰
        q = q.replace('é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°ï¼Œæ‚¨æŠ•è³‡çš„å…¬å¸åœ¨å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•çš„é€²åº¦ç‚ºä½•ï¼Ÿ', 
                     'å…¬å¸é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•é€²åº¦')
        q = q.replace('å…¬å¸é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°ï¼Œå»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•çš„é€²åº¦ç‚ºä½•ï¼Ÿ', 
                     'å…¬å¸é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•é€²åº¦')
        q = q.replace('é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°ï¼Œæ‚¨æŠ•è³‡çš„å…¬å¸åœ¨å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•çš„é€²åº¦ç‚ºä½•', 
                     'å…¬å¸é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•é€²åº¦')
        q = q.replace('å…¬å¸é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°ï¼Œå»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•çš„é€²åº¦ç‚ºä½•', 
                     'å…¬å¸é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•é€²åº¦')
    
    # 1. ç§»é™¤ã€Œæœªå‘½åé¡Œç›® - ã€å‰ç¶´
    q = re.sub(r'^æœªå‘½åé¡Œç›®[\s\-ï¼â€”â€“ï¼š:]*', '', q)
    
    # 2. çµ±ä¸€å¡«ç©ºç¬¦è™Ÿï¼ˆå…ˆè™•ç†ï¼Œé¿å…å¾ŒçºŒè¢«èª¤åˆªï¼‰
    q = re.sub(r'_{2,}', ' _ ', q)
    q = re.sub(r'\([\s_]*\)', ' _ ', q)
    q = re.sub(r'ï¼ˆ[\s_]*ï¼‰', ' _ ', q)
    
    # 3. çµ±ä¸€ã€Œè‘£ç›£äº‹ã€ç›¸é—œè©å½™ï¼ˆæå‰è™•ç†ï¼‰
    q = q.replace('è‘£ç›£äº‹å¸­æ¬¡', 'è‘£äº‹å¸­æ¬¡')
    q = q.replace('è‘£ç›£äº‹ _ ä½', 'è‘£äº‹ _ ä½')
    
    # 4. ç§»é™¤ã€Œåœ¨...æ–¹é¢ã€ã€ã€Œåœ¨...ä¸Šã€ç­‰ä»‹ç³»è©ç‰‡èª
    q = re.sub(r'åœ¨(.{1,15}?)æ–¹é¢', r'\1', q)
    q = re.sub(r'åœ¨(.{1,15}?)ä¸Š', r'\1', q)
    
    # 5. çµ±ä¸€ã€Œå…¶ã€ã€ã€Œçš„ã€ã€ã€Œç›®å‰çš„ã€ã€ã€Œä¹‹ã€ç­‰èªæ°£è©
    q = q.replace('å…¶å®šæœŸæ€§è‘£äº‹æœƒ', 'å®šæœŸæ€§è‘£äº‹æœƒ')
    q = q.replace('å…¶è‘£äº‹æœƒ', 'è‘£äº‹æœƒ')
    q = q.replace('å…¶è‚¡æ±çµæ§‹', 'è‚¡æ±çµæ§‹')
    q = q.replace('å…¶è‘£äº‹åŠç¶“ç†äºº', 'è‘£äº‹åŠç¶“ç†äºº')
    q = q.replace('å…¶å“¡å·¥äººæ•¸', 'å“¡å·¥äººæ•¸')
    q = q.replace('å…¶å“¡å·¥åˆ†ç´…', 'å“¡å·¥åˆ†ç´…')
    q = q.replace('çš„å®šæœŸæ€§è‘£äº‹æœƒ', 'å®šæœŸæ€§è‘£äº‹æœƒ')
    q = q.replace('çš„è‚¡æ±çµæ§‹', 'è‚¡æ±çµæ§‹')
    q = q.replace('çš„è‘£äº‹é–“', 'è‘£äº‹é–“')
    q = q.replace('ç›®å‰çš„è‘£äº‹å¸­æ¬¡', 'è‘£äº‹å¸­æ¬¡')
    q = q.replace('ç›®å‰çš„ç›£å¯Ÿäººå¸­æ¬¡', 'ç›£å¯Ÿäººå¸­æ¬¡')
    q = q.replace('ç›®å‰çš„', '')
    q = q.replace('ä¹‹è‘£äº‹é•·', 'è‘£äº‹é•·')
    q = q.replace('ä¹‹è‘£äº‹æœƒ', 'è‘£äº‹æœƒ')
    q = q.replace('ä¹‹è‘£äº‹', 'è‘£äº‹')
    q = q.replace('ä¹‹ç›£å¯Ÿäºº', 'ç›£å¯Ÿäºº')
    q = q.replace('ä¹‹å¤§è‚¡æ±', 'å¤§è‚¡æ±')
    q = q.replace('ä¹‹ç¶“ç‡Ÿåœ˜éšŠ', 'ç¶“ç‡Ÿåœ˜éšŠ')
    q = q.replace('ä¹‹ç¾é‡‘æµé‡', 'ç¾é‡‘æµé‡')
    
    # 6. è£œå……ç¼ºå¤±çš„ä¸»é¡Œæ¨™ç±¤
    if ' - ' not in q and 'æ­éœ²è‘£äº‹çš„å€‹åˆ¥é…¬é‡‘' in q:
        q = 'è³‡è¨Šé€æ˜åº¦ - ' + q
    if ' - ' not in q and 'æ­éœ²ç¸½ç¶“ç†åŠå‰¯ç¸½ç¶“ç†çš„å€‹åˆ¥é…¬é‡‘' in q:
        q = 'è³‡è¨Šé€æ˜åº¦ - ' + q
    if ' - ' not in q and 'è‘£äº‹åŠç¶“ç†äººçš„é…¬é‡‘èˆ‡å…¬å¸ç¸¾æ•ˆé€£å‹•' in q:
        q = 'è³‡è¨Šé€æ˜åº¦ - ' + q
    if ' - ' not in q and 'è«®è©¢é¡§å•' in q and 'é »ç‡' in q:
        q = 'è‘£äº‹æœƒçµæ§‹èˆ‡é‹ä½œ - ' + q
    if ' - ' not in q and ('è‘£äº‹å¸­æ¬¡' in q or 'ç›£å¯Ÿäººå¸­æ¬¡' in q):
        q = 'è‘£äº‹æœƒçµæ§‹èˆ‡é‹ä½œ - ' + q
    
    # 7. çµ±ä¸€èº«åˆ†ç›¸é—œè©å½™ï¼ˆæ›´å…¨é¢çš„æ›¿æ›ï¼‰
    identity_replacements = [
        # === æœ€é«˜å„ªå…ˆï¼šç²¾ç¢ºå®Œæ•´åŒ¹é…ï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„è®Šé«”ï¼‰===
        # å…§éƒ¨æ§åˆ¶å¾ªç’°é¡Œç›®ï¼ˆç‰¹æ®Šè™•ç†ï¼šæŠ•è³‡æ–¹ç‰ˆæœ¬ç¼ºå°‘é …ç›®åˆ—è¡¨ï¼‰
        ('é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°ï¼Œæ‚¨æŠ•è³‡çš„å…¬å¸åœ¨å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•çš„é€²åº¦ç‚ºä½•ï¼Ÿ', 'å…¬å¸é‡å°ä¸‹åˆ—å…§éƒ¨æ§åˆ¶å¾ªç’°å»ºç«‹æ›¸é¢æ§åˆ¶ç¨‹åºèˆ‡åŸ·è¡Œè‡ªè©•é€²åº¦'),
        # === ä»¥ä¸Šç‚ºæ–°å¢ ===
        # ç‰¹å®šå¥å‹å„ªå…ˆè™•ç†ï¼ˆæ›´è©³ç´°çš„å°æ‡‰ï¼‰
        ('æ‚¨ä¸»è¦æŠ•è³‡çš„æœªä¸Šå¸‚ï¼ˆæ«ƒï¼‰å…¬å¸æ‰€å±¬ç”¢æ¥­é¡åˆ¥', 'ä¸»è¦ç”¢æ¥­é¡åˆ¥'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸å…¶å“¡å·¥äººæ•¸', 'å“¡å·¥äººæ•¸'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸å…¶å“¡å·¥åˆ†ç´…', 'å…¬å¸å“¡å·¥åˆ†ç´…'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸å…¶è‚¡æ±çµæ§‹ä¸­åŒ…å«æ³•äººè‚¡æ±ï¼ˆå¦‚å‰µæŠ•ï¼‰', 'å…¬å¸è‚¡æ±çµæ§‹ä¸­åŒ…å«æ³•äººè‚¡æ±'),
        ('å…¬å¸çš„è‚¡æ±çµæ§‹ä¸­åŒ…å«æ³•äººè‚¡æ±æˆ–å‰µæŠ•', 'å…¬å¸è‚¡æ±çµæ§‹ä¸­åŒ…å«æ³•äººè‚¡æ±'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸åœ¨ç¾é‡‘æµé‡è¦åŠƒèˆ‡ç›£æ§åˆ¶åº¦çš„å»ºç«‹ç¨‹åº¦å¦‚ä½•', 'å…¬å¸ç¾é‡‘æµé‡è¦åŠƒèˆ‡ç›£æ§åˆ¶åº¦å»ºç«‹ç¨‹åº¦'),
        ('æ‚¨èªç‚ºå…¬å¸ç¾é‡‘æµé‡è¦åŠƒèˆ‡ç›£æ§åˆ¶åº¦çš„å»ºç«‹ç¨‹åº¦å¦‚ä½•', 'å…¬å¸ç¾é‡‘æµé‡è¦åŠƒèˆ‡ç›£æ§åˆ¶åº¦å»ºç«‹ç¨‹åº¦'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸åœ¨å»ºç«‹æ›¸é¢æ ¸å‡†æµç¨‹æœ‰å›°é›£', 'å…¬å¸å»ºç«‹æ›¸é¢æ ¸å‡†æµç¨‹æ˜¯æŒ‘æˆ°'),
        ('å»ºç«‹æ›¸é¢æ ¸å‡†æµç¨‹å°å…¬å¸ä¾†èªªæ˜¯ä¸€é …æŒ‘æˆ°', 'å…¬å¸å»ºç«‹æ›¸é¢æ ¸å‡†æµç¨‹æ˜¯æŒ‘æˆ°'),
        ('æ‰¿ä¸Šé¡Œï¼Œæ‚¨æŠ•è³‡çš„å…¬å¸ä¹‹ç¾é‡‘æµé‡è¶³ä»¥æ”¯æ’å…¬å¸ç‡Ÿé‹å¹¾å€‹æœˆ', 'æ‰¿ä¸Šé¡Œå…¬å¸ç¾é‡‘æµé‡è¶³ä»¥æ”¯æ’å…¬å¸ç‡Ÿé‹å¹¾å€‹æœˆ'),
        ('æ‰¿ä¸Šé¡Œæ‚¨èªç‚ºå…¬å¸ç¾é‡‘æµé‡è¶³ä»¥æ”¯æ’å…¬å¸ç‡Ÿé‹å¹¾å€‹æœˆ', 'æ‰¿ä¸Šé¡Œå…¬å¸ç¾é‡‘æµé‡è¶³ä»¥æ”¯æ’å…¬å¸ç‡Ÿé‹å¹¾å€‹æœˆ'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸æœ‰æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²è‘£äº‹çš„å€‹åˆ¥é…¬é‡‘', 'å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²è‘£äº‹çš„å€‹åˆ¥é…¬é‡‘'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸æœ‰æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²ç¸½ç¶“ç†åŠå‰¯ç¸½ç¶“ç†çš„å€‹åˆ¥é…¬é‡‘', 'å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²ç¸½ç¶“ç†åŠå‰¯ç¸½ç¶“ç†çš„å€‹åˆ¥é…¬é‡‘'),
        ('è«‹å•æ‚¨æŠ•è³‡çš„å…¬å¸ä¹‹å¤§è‚¡æ±ï¼ˆæŒè‚¡5%ä»¥ä¸Šï¼‰äººæ•¸æœ‰å¤šå°‘äºº', 'å…¬å¸å¤§è‚¡æ±ï¼ˆæŒè‚¡5%ä»¥ä¸Šï¼‰äººæ•¸'),
        ('è«‹å•å…¬å¸çš„å¤§è‚¡æ±ï¼ˆæŒè‚¡5%ä»¥ä¸Šï¼‰äººæ•¸å¤šå°‘äºº', 'å…¬å¸å¤§è‚¡æ±ï¼ˆæŒè‚¡5%ä»¥ä¸Šï¼‰äººæ•¸'),
        ('è«‹å•æ‚¨æŠ•è³‡çš„å…¬å¸ä¹‹å¤§è‚¡æ±', 'å…¬å¸å¤§è‚¡æ±'),
        ('è«‹å•æ‚¨æŠ•è³‡çš„å…¬å¸', 'å…¬å¸'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸åœ¨éå»12å€‹æœˆå…§ï¼Œè‘£äº‹æœƒçš„å¬é–‹é »ç‡ç‚ºä½•', 'å…¬å¸éå»12å€‹æœˆå…§ï¼Œè‘£äº‹æœƒå¬é–‹é »ç‡'),
        ('åœ¨éå»12å€‹æœˆå…§ï¼Œè²´å…¬å¸è‘£äº‹æœƒçš„å¬é–‹é »ç‡ç‚ºä½•', 'å…¬å¸éå»12å€‹æœˆå…§ï¼Œè‘£äº‹æœƒå¬é–‹é »ç‡'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸', 'å…¬å¸'),
        ('è«‹å¡«å¯«å…¬å¸è‘£äº‹å¸­æ¬¡', 'å…¬å¸è‘£äº‹å¸­æ¬¡'),
        ('è«‹å¡«å¯«å…¬å¸ç›£å¯Ÿäººå¸­æ¬¡', 'å…¬å¸ç›£å¯Ÿäººå¸­æ¬¡'),
        ('è«‹å¡«å¯«å…¬å¸è‘£ç›£äº‹å¸­æ¬¡', 'å…¬å¸è‘£äº‹å¸­æ¬¡'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸å…¶å®šæœŸæ€§è‘£äº‹æœƒçš„è­°äº‹å…§å®¹', 'å…¬å¸å®šæœŸæ€§è‘£äº‹æœƒçš„è­°äº‹å…§å®¹'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸å®šæœŸæ€§è‘£äº‹æœƒçš„è­°äº‹å…§å®¹', 'å…¬å¸å®šæœŸæ€§è‘£äº‹æœƒçš„è­°äº‹å…§å®¹'),
        ('å…¬å¸å®šæœŸæ€§è‘£äº‹æœƒçš„è­°äº‹å…§å®¹', 'å…¬å¸å®šæœŸæ€§è‘£äº‹æœƒçš„è­°äº‹å…§å®¹'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸æœ‰æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²', 'å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²', 'å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²'),
        ('è²´å…¬å¸æœ‰æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²', 'å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²'),
        ('è²´å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²', 'å…¬å¸æ¸…æ¥šçš„å‘è‚¡æ±æ­éœ²'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸åœ¨è«®è©¢é¡§å•', 'å…¬å¸è«®è©¢é¡§å•'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸è«®è©¢é¡§å•', 'å…¬å¸è«®è©¢é¡§å•'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸åœ¨è¨‚å®šè²¡æœƒä½œæ¥­ç¨‹åºä¸Šæœƒ', 'å…¬å¸è¨‚å®šè²¡æœƒä½œæ¥­ç¨‹åº'),
        ('è¨‚å®šè²¡æœƒä½œæ¥­ç¨‹åºå°å…¬å¸ä¾†èªª', 'å…¬å¸è¨‚å®šè²¡æœƒä½œæ¥­ç¨‹åº'),
        ('è«‹å¡«å¯«å…¬å¸', 'å…¬å¸'),
        ('è²´å…¬å¸è‘£äº‹æœƒ', 'å…¬å¸è‘£äº‹æœƒ'),
        ('è²´å…¬å¸', 'å…¬å¸'),
        # é€šç”¨æ›¿æ›
        ('æ‚¨æŠ•è³‡çš„å…¬å¸æœ‰', 'å…¬å¸'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸å…¶', 'å…¬å¸'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸åœ¨', 'å…¬å¸'),
        ('æ‚¨æŠ•è³‡çš„å…¬å¸æœƒ', 'å…¬å¸'),
        ('è²´å…¬å¸æœ‰', 'å…¬å¸'),
        ('è²´å…¬å¸åœ¨', 'å…¬å¸'),
        ('æ‚¨èªç‚ºå…¬å¸', 'å…¬å¸'),
        ('æ‚¨èªç‚º', ''),
        ('è«‹å•å…¬å¸', 'å…¬å¸'),
        ('è«‹å¡«å¯«', ''),
    ]
    
    for old, new in identity_replacements:
        q = q.replace(old, new)
    
    # 8. ç§»é™¤é¡Œç›®é–‹é ­çš„å†—é¤˜å‰ç¶´ï¼ˆä¿®æ­£æ­£å‰‡è¡¨é”å¼ï¼‰
    q = re.sub(r'^(å…¬å¸æ–¹[\s\-ï¼â€”â€“ï¼š:]+|æŠ•è³‡æ–¹[\s\-ï¼â€”â€“ï¼š:]+|è«‹å•[\s\-ï¼â€”â€“ï¼š:]*|è«‹å¡«å¯«[\s\-ï¼â€”â€“ï¼š:]*)', '', q)
    
    # 9. çµ±ä¸€å†’è™Ÿå’Œã€Œä½ã€çš„æ ¼å¼
    q = q.replace('ï¼š è‘£äº‹', 'ï¼šè‘£äº‹')
    q = q.replace(': è‘£äº‹', 'ï¼šè‘£äº‹')
    q = q.replace('ï¼š ç›£å¯Ÿäºº', 'ï¼šç›£å¯Ÿäºº')
    q = q.replace(': ç›£å¯Ÿäºº', 'ï¼šç›£å¯Ÿäºº')
    q = re.sub(r'ï¼š[\s]+è‘£äº‹', 'ï¼šè‘£äº‹', q)
    q = re.sub(r'ï¼š[\s]+ç›£å¯Ÿäºº', 'ï¼šç›£å¯Ÿäºº', q)
    
    # 10. çµ±ä¸€ã€Œé »ç‡ç‚ºä½•ã€ã€ã€Œç‚ºä½•ã€ã€ã€Œå¦‚ä½•ã€ã€ã€Œå¤šå°‘äººã€ç­‰å•å¥
    q = q.replace('çš„å¬é–‹é »ç‡ç‚ºä½•', 'å¬é–‹é »ç‡')
    q = q.replace('å¬é–‹é »ç‡ç‚ºä½•', 'å¬é–‹é »ç‡')
    q = q.replace('çš„é »ç‡ç‚ºä½•ï¼Ÿ', 'é »ç‡')
    q = q.replace('é »ç‡ç‚ºä½•ï¼Ÿ', 'é »ç‡')
    q = q.replace('ç‚ºä½•ï¼Ÿ', '')
    q = q.replace('å¦‚ä½•ï¼Ÿ', '')
    q = q.replace('çš„é »ç‡', 'é »ç‡')
    q = q.replace('çš„å»ºç«‹ç¨‹åº¦å¦‚ä½•', 'å»ºç«‹ç¨‹åº¦')
    q = q.replace('å»ºç«‹ç¨‹åº¦å¦‚ä½•', 'å»ºç«‹ç¨‹åº¦')
    q = q.replace('çš„é€²åº¦ç‚ºä½•', 'é€²åº¦')
    q = q.replace('é€²åº¦ç‚ºä½•', 'é€²åº¦')
    q = q.replace('äººæ•¸æœ‰å¤šå°‘äºº', 'äººæ•¸')
    q = q.replace('äººæ•¸å¤šå°‘äºº', 'äººæ•¸')
    q = q.replace('æœ‰å¤šå°‘äºº', '')
    q = q.replace('å¤šå°‘äºº', '')
    
    # 11. çµ±ä¸€æ¨™é»ç¬¦è™Ÿ
    q = q.replace('ï¼', ' - ').replace('â€”', ' - ').replace('â€“', ' - ')
    q = q.replace('ï¼š', ':').replace('ã€‚', '.')
    q = q.replace('ï¼Ÿ', '').replace('?', '')
    
    # 12. çµ±ä¸€æ‹¬è™Ÿèˆ‡è¤‡é¸æ¨™è¨˜
    q = q.replace('(å¯è¤‡é¸)', '').replace('ï¼ˆå¯è¤‡é¸ï¼‰', '')
    q = q.replace('(è¤‡é¸)', '').replace('ï¼ˆè¤‡é¸ï¼‰', '')
    q = q.replace('ï¼ˆå¦‚å‰µæŠ•ï¼‰', '')
    q = q.replace('æˆ–å‰µæŠ•', '')
    
    # ç§»é™¤æ‹¬è™Ÿå…§çš„è©³ç´°èªªæ˜ï¼ˆåŒ…å«å¤šå€‹ç©ºæ ¼çš„æƒ…æ³ï¼‰
    q = re.sub(r'\s{2,}\([^\)]+\)', '', q)
    q = re.sub(r'\s*\([^\)]{10,}\)', '', q)
    q = re.sub(r'\s*ï¼ˆ[^ï¼‰]{10,}ï¼‰', '', q)
    
    # 13. ç§»é™¤ã€Œä½ã€å‰çš„å¤šé¤˜ç©ºæ ¼å’Œç¬¦è™Ÿ
    q = re.sub(r'[\s_]+ä½', 'ä½', q)
    
    # 14. çµ±ä¸€ã€Œæ˜¯/æœƒ/æœ‰ã€ç­‰åŠ©å‹•è©å’Œèªæ°£è©
    q = q.replace('ä¾†èªªæ˜¯', '')
    q = q.replace('å°å…¬å¸ä¾†èªªæ˜¯ä¸€é …æŒ‘æˆ°', 'æ˜¯æŒ‘æˆ°')
    q = q.replace('å°å…¬å¸ä¾†èªªæ˜¯ä¸å°çš„è² æ“”', 'æ˜¯è² æ“”')
    q = q.replace('ä¸Šæœƒæ˜¯', '')
    q = q.replace('æœƒæ˜¯', '')
    q = q.replace('æœ‰å›°é›£', 'æ˜¯æŒ‘æˆ°')
    q = q.replace('æ˜¯ä¸å°çš„è² æ“”', 'æ˜¯è² æ“”')
    
    # 15. ç§»é™¤å¤šé¤˜ç©ºç™½
    q = re.sub(r'\s+', ' ', q).strip()
    
    # 16. ç§»é™¤å°¾éƒ¨æ¨™é»
    q = q.rstrip('ï¼š:ã€‚.,;ï¼›ï¼Ÿ?')
    
    return q

def calculate_similarity(s1, s2):
    """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ (0-1)ï¼Œè€ƒæ…®æ ¸å¿ƒå…§å®¹å·®ç•°"""
    # ä½¿ç”¨ SequenceMatcher è¨ˆç®—åŸºç¤ç›¸ä¼¼åº¦
    base_similarity = SequenceMatcher(None, s1, s2).ratio()
    
    # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œé€²ä¸€æ­¥æª¢æŸ¥é—œéµè©å·®ç•°
    if base_similarity > 0.8:
        # æå–é—œéµåè©ï¼ˆé¿å…èª¤åˆä½µä¸åŒä¸»é¡Œçš„é¡Œç›®ï¼‰
        keywords_s1 = set(re.findall(r'[\u4e00-\u9fff]{2,}', s1))
        keywords_s2 = set(re.findall(r'[\u4e00-\u9fff]{2,}', s2))
        
        # è¨ˆç®—é—œéµè©äº¤é›†æ¯”ä¾‹
        if keywords_s1 and keywords_s2:
            keyword_overlap = len(keywords_s1 & keywords_s2) / max(len(keywords_s1), len(keywords_s2))
            # èª¿æ•´ç›¸ä¼¼åº¦ï¼šå¦‚æœé—œéµè©å·®ç•°å¤§ï¼Œé™ä½ç›¸ä¼¼åº¦
            return base_similarity * (0.5 + 0.5 * keyword_overlap)
    
    return base_similarity

def merge_similar_questions(df, cols_to_exclude, similarity_threshold=0.75):  # é™ä½åˆ° 0.75
    """
    åŸºæ–¼ç›¸ä¼¼åº¦åˆä½µé¡Œç›®ï¼ˆæ›´ç©æ¥µè™•ç†ã€Œæœªå‘½åé¡Œç›®ã€èˆ‡å–®æ–¹é¡Œç›®ï¼‰
    
    Returns:
        - merged_mapping: {ä»£è¡¨é¡Œç›®: [æ‰€æœ‰åŸå§‹é¡Œç›®]}
        - cols_to_analyze: å»é‡å¾Œçš„é¡Œç›®åˆ—è¡¨
    """
    all_cols = [c for c in df.columns if c not in cols_to_exclude]
    
    # ç¬¬ä¸€æ­¥ï¼šæ¨™æº–åŒ–ä¸¦åˆ†çµ„ï¼ˆæ¨™æº–åŒ–å¾Œç›¸åŒçš„é¡Œç›®æœƒè‡ªå‹•åˆä½µï¼‰
    normalized_groups = {}
    for col in all_cols:
        norm = normalize_question_v2(col)
        if norm not in normalized_groups:
            normalized_groups[norm] = []
        normalized_groups[norm].append(col)
    
    # ç¬¬äºŒæ­¥ï¼šç›¸ä¼¼åº¦åŒ¹é…ï¼ˆè™•ç†æ¨™æº–åŒ–å¾Œä»æœ‰ç´°å¾®å·®ç•°çš„æƒ…æ³ï¼‰
    merged_mapping = {}
    processed = set()
    
    norm_keys = list(normalized_groups.keys())
    for i, norm1 in enumerate(norm_keys):
        if norm1 in processed:
            continue
        
        # æ‰¾å‡ºæ‰€æœ‰ç›¸ä¼¼çš„æ¨™æº–åŒ–é¡Œç›®ï¼ˆåŒ…æ‹¬ norm1 æœ¬èº«ï¼‰
        similar_group = [norm1]
        for norm2 in norm_keys[i+1:]:
            if norm2 in processed:
                continue
            similarity = calculate_similarity(norm1, norm2)
            if similarity >= similarity_threshold:
                similar_group.append(norm2)
                processed.add(norm2)
        
        # åˆä½µæ‰€æœ‰ç›¸ä¼¼é¡Œç›®çš„åŸå§‹æ¬„ä½
        all_originals = []
        for norm in similar_group:
            all_originals.extend(normalized_groups[norm])
        
        # å„ªå…ˆé¸æ“‡æ²’æœ‰ã€Œæœªå‘½åé¡Œç›®ã€ä¸”è¼ƒçŸ­çš„ä½œç‚ºä»£è¡¨ï¼ˆå…¬å¸æ–¹å„ªå…ˆï¼‰
        representative = None
        for orig in sorted(all_originals, key=lambda x: (len(x), 'æŠ•è³‡' in x)):
            if 'æœªå‘½åé¡Œç›®' not in orig:
                representative = orig
                break
        if representative is None:  # å¦‚æœå…¨éƒ¨éƒ½æ˜¯æœªå‘½åé¡Œç›®
            representative = all_originals[0]
        
        merged_mapping[representative] = all_originals
        processed.add(norm1)
    
    # ç¬¬ä¸‰æ­¥ï¼šè³‡æ–™åˆä½µ
    for representative, originals in merged_mapping.items():
        if len(originals) > 1:
            for other_col in originals[1:]:
                # å„ªå…ˆä¿ç•™ä»£è¡¨é¡Œç›®çš„è³‡æ–™ï¼Œç”¨å…¶ä»–é¡Œç›®å¡«è£œç¼ºå¤±
                mask = df[representative].isna() & df[other_col].notna()
                df.loc[mask, representative] = df.loc[mask, other_col]

    cols_to_analyze = list(merged_mapping.keys())
    return merged_mapping, cols_to_analyze

def generate_report_recommendations(df, cols_to_analyze, analysis_mode):
    """åˆ†æä¸¦æ¨è–¦å€¼å¾—ç´å…¥å ±å‘Šçš„é¡Œç›®"""
    recommendations = []
    processed_cols = set()
    
    for col_name in cols_to_analyze:
        if col_name not in df.columns or col_name in processed_cols:
            continue
        
        processed_cols.add(col_name)
        col_series = df[col_name].dropna()
        if col_series.empty or len(col_series) < 5:
            continue
        
        recommendation = {
            'é¡Œç›®': col_name[:80] + '...' if len(col_name) > 80 else col_name,
            'å®Œæ•´é¡Œç›®': col_name,
            'æ¨£æœ¬æ•¸': int(df[col_name].notna().sum()),
            'ç¼ºå¤±ç‡': f"{(df[col_name].isna().sum() / len(df) * 100):.1f}%",
            'æ¨è–¦ç†ç”±': [],
            'å„ªå…ˆé †åº': 0.0,
            'çµ±è¨ˆçµæœ': {}
        }
        
        is_multiselect = col_series.dtype == 'object' and col_series.astype(str).str.contains('\n', na=False).any()
        
        # åªåœ¨åˆä½µåˆ†æä¸”æœ‰ respondent_type æ™‚é€²è¡Œæ¯”è¼ƒæª¢å®š
        if analysis_mode == 'åˆä½µåˆ†æ' and 'respondent_type' in df.columns:
            try:
                if is_multiselect:
                    exploded = col_series.astype(str).str.split('\n').explode().str.strip()
                    exploded = exploded[(exploded != '') & (exploded != 'nan') & exploded.notna()]
                    if not exploded.empty:
                        total_counts = exploded.value_counts()
                        significant_count = 0
                        for opt in total_counts.index[:10]:
                            if pd.isna(opt) or str(opt).lower() == 'nan':
                                continue
                            pres = df[col_name].astype(str).fillna('').apply(
                                lambda s: opt in [x.strip() for x in s.split('\n') if x.strip()!='' and x.strip().lower()!='nan']
                            )
                            table = pd.crosstab(pres, df['respondent_type'])
                            if table.size > 0 and table.values.sum() > 0 and table.shape[0] >= 2:
                                try:
                                    chi2, p, dof, exp = chi2_contingency(table)
                                    if np.nanmin(exp) > 1 and p < 0.05:
                                        significant_count += 1
                                        recommendation['çµ±è¨ˆçµæœ'].setdefault('é¡¯è‘—é¸é …', []).append({'é¸é …': opt, 'p': p})
                                        if p < 0.001:
                                            recommendation['å„ªå…ˆé †åº'] += 3
                                        elif p < 0.01:
                                            recommendation['å„ªå…ˆé †åº'] += 2
                                        else:
                                            recommendation['å„ªå…ˆé †åº'] += 1
                                except Exception:
                                    pass
                        if significant_count > 0:
                            recommendation['æ¨è–¦ç†ç”±'].append(f"æœ‰ {significant_count} å€‹é¸é …åœ¨å…¬å¸æ–¹/æŠ•è³‡æ–¹é–“å‘ˆç¾çµ±è¨ˆé¡¯è‘—å·®ç•°")
                            recommendation['çµ±è¨ˆçµæœ']['é¡¯è‘—é¸é …æ•¸'] = significant_count
                else:
                    is_numeric = pd.api.types.is_numeric_dtype(col_series)
                    if not is_numeric:
                        numeric_version = pd.to_numeric(col_series, errors='coerce').dropna()
                        if len(numeric_version) > 0 and (len(numeric_version) / len(col_series) > 0.7):
                            is_numeric = True
                            col_num = numeric_version
                        else:
                            is_numeric = False
                    else:
                        col_num = pd.to_numeric(col_series, errors='coerce').dropna()
                    
                    if is_numeric:
                        groups = []
                        for rt in df['respondent_type'].unique():
                            grp = col_num[df.loc[col_num.index, 'respondent_type'] == rt]
                            if len(grp) > 0:
                                groups.append(grp.astype(float))
                        if len(groups) == 2:
                            try:
                                stat, p = mannwhitneyu(groups[0], groups[1], alternative='two-sided')
                                median_diff = abs(np.median(groups[0]) - np.median(groups[1]))
                                recommendation['çµ±è¨ˆçµæœ']['p'] = float(p)
                                recommendation['çµ±è¨ˆçµæœ']['median_diff'] = float(median_diff)
                                if p < 0.05:
                                    recommendation['æ¨è–¦ç†ç”±'].append(f"å…¬å¸æ–¹/æŠ•è³‡æ–¹ä¸­ä½æ•¸å·®ç•°é¡¯è‘— (p={p:.3f})")
                                    recommendation['å„ªå…ˆé †åº'] += 2
                            except Exception:
                                pass
                        elif len(groups) > 2:
                            try:
                                stat, p = kruskal(*groups)
                                if p < 0.05:
                                    recommendation['æ¨è–¦ç†ç”±'].append("è·¨çµ„å·®ç•°é¡¯è‘— (Kruskal-Wallis)")
                                    recommendation['å„ªå…ˆé †åº'] += 2
                            except Exception:
                                pass
                    else:
                        s = col_series.astype(str)
                        s = s[~s.str.lower().str.contains('nan', na=False)]
                        if not s.empty:
                            table = pd.crosstab(s, df.loc[s.index, 'respondent_type'])
                            if table.size > 0 and table.values.sum() > 0:
                                try:
                                    if table.shape == (2, 2) and table.values.sum() < 20:
                                        oddsratio, p = fisher_exact(table)
                                    else:
                                        chi2, p, dof, exp = chi2_contingency(table)
                                    
                                    if p < 0.05:
                                        recommendation['æ¨è–¦ç†ç”±'].append(f"å…¬å¸æ–¹/æŠ•è³‡æ–¹åˆ†ä½ˆé¡¯è‘—å·®ç•° (p={p:.3f})")
                                        recommendation['çµ±è¨ˆçµæœ']['p'] = float(p)
                                        if p < 0.001:
                                            recommendation['å„ªå…ˆé †åº'] += 3
                                        elif p < 0.01:
                                            recommendation['å„ªå…ˆé †åº'] += 2
                                        else:
                                            recommendation['å„ªå…ˆé †åº'] += 1
                                except Exception:
                                    pass
            except Exception:
                pass
        
        # é¡å¤–è©•åˆ†æ¨™æº–
        missing_rate = df[col_name].isna().sum() / len(df)
        if missing_rate < 0.05:
            recommendation['æ¨è–¦ç†ç”±'].append("è³‡æ–™å®Œæ•´åº¦é«˜ (ç¼ºå¤± < 5%)")
            recommendation['å„ªå…ˆé †åº'] += 1
        
        if not is_multiselect and not col_series.empty:
            unique_ratio = len(col_series.unique()) / len(col_series)
            if unique_ratio > 0.3:
                recommendation['æ¨è–¦ç†ç”±'].append("ç­”æ¡ˆå…·å¤šæ¨£æ€§")
                recommendation['å„ªå…ˆé †åº'] += 0.5
        
        if recommendation['æ¨è–¦ç†ç”±']:
            recommendations.append(recommendation)
    
    recommendations.sort(key=lambda x: x['å„ªå…ˆé †åº'], reverse=True)
    return recommendations

def generate_professional_report(df, recommendations, cols_to_analyze, analysis_mode):
    """
    ç”Ÿæˆç¬¦åˆåœ‹ç™¼åŸºé‡‘éœ€æ±‚çš„å°ˆæ¥­åˆ†æå ±å‘Š
    çµæ§‹ï¼šåŸ·è¡Œæ‘˜è¦ â†’ æ–¹æ³•è«– â†’ ä¸»è¦ç™¼ç¾ â†’ çµè«–èˆ‡å»ºè­°
    """
    report = []
    
    # === 1. æ¨™é¡Œèˆ‡åŸºæœ¬è³‡è¨Š ===
    report.append("# æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·åˆ†æå ±å‘Š")
    report.append(f"\n**å ±å‘Šç”¢ç”Ÿæ™‚é–“ï¼š** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}")
    report.append(f"\n**åˆ†ææ¨¡å¼ï¼š** {analysis_mode}")
    report.append(f"\n**ç¸½æ¨£æœ¬æ•¸ï¼š** {len(df)} ç­†")
    
    if 'respondent_type' in df.columns:
        respondent_counts = df['respondent_type'].value_counts()
        report.append(f"\n**å¡«ç­”è€…åˆ†ä½ˆï¼š**")
        for resp_type, count in respondent_counts.items():
            report.append(f"- {resp_type}ï¼š{count} ç­† ({count/len(df)*100:.1f}%)")
    
    if 'phase' in df.columns and df['phase'].notna().any():
        phase_counts = df['phase'].value_counts()
        report.append(f"\n**éšæ®µåˆ†ä½ˆï¼š**")
        for phase, count in phase_counts.items():
            report.append(f"- {phase}ï¼š{count} ç­† ({count/len(df)*100:.1f}%)")
    
    report.append("\n---\n")
    
    # === 2. åŸ·è¡Œæ‘˜è¦ ===
    report.append("## ğŸ“‹ åŸ·è¡Œæ‘˜è¦\n")
    report.append("æœ¬å ±å‘Šé‡å°æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·é€²è¡Œå…¨é¢æ€§çµ±è¨ˆåˆ†æï¼Œä¸»è¦ç›®çš„åœ¨æ–¼ç­è§£å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹å°å…¬å¸æ²»ç†å¯¦å‹™çš„èªçŸ¥å·®ç•°ï¼Œä»¥åŠä¸åŒéšæ®µå…¬å¸åœ¨æ²»ç†é¢å‘çš„ç™¼å±•ç‹€æ³ã€‚\n")
    
    # æ‰¾å‡ºæœ€é‡è¦çš„3-5å€‹ç™¼ç¾
    top_findings = recommendations[:min(5, len(recommendations))]
    report.append("### é—œéµç™¼ç¾ï¼š\n")
    for idx, rec in enumerate(top_findings, 1):
        topic = rec['å®Œæ•´é¡Œç›®']
        priority = rec['å„ªå…ˆé †åº']
        reasons = rec['æ¨è–¦ç†ç”±']
        
        # å°‡çµ±è¨ˆè¡“èªè½‰ç‚ºæ¥­å‹™èªè¨€
        business_insight = []
        for reason in reasons:
            if "å…¬å¸æ–¹/æŠ•è³‡æ–¹" in reason and "é¡¯è‘—å·®ç•°" in reason:
                business_insight.append("**å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹å°æ­¤è­°é¡Œçš„çœ‹æ³•å­˜åœ¨é¡¯è‘—è½å·®**ï¼Œå»ºè­°é—œæ³¨é›™æ–¹èªçŸ¥å·®ç•°çš„æ ¹æº")
            elif "åˆ†ä½ˆé¡¯è‘—å·®ç•°" in reason:
                business_insight.append("**ä¸åŒç¾¤é«”åœ¨æ­¤è­°é¡Œä¸Šå‘ˆç¾æ˜é¡¯å·®ç•°**ï¼Œå€¼å¾—é€²ä¸€æ­¥æ¢è¨é€ æˆå·®ç•°çš„å› ç´ ")
            elif "è³‡æ–™å®Œæ•´åº¦é«˜" in reason:
                business_insight.append("æ­¤è­°é¡Œç²å¾—é«˜åº¦é—œæ³¨ï¼Œè³‡æ–™å“è³ªå„ªè‰¯")
            elif "ç­”æ¡ˆå…·å¤šæ¨£æ€§" in reason:
                business_insight.append("å—è¨ªè€…å›æ‡‰å…·å¤šæ¨£æ€§ï¼Œåæ˜ å¯¦å‹™åšæ³•çš„å¤šå…ƒæ€§")
        
        report.append(f"{idx}. **{topic[:60]}{'...' if len(topic) > 60 else ''}**")
        report.append(f"   - é‡è¦æ€§è©•åˆ†ï¼š{priority:.1f} åˆ†")
        if business_insight:
            report.append(f"   - æ¥­å‹™æ„æ¶µï¼š{business_insight[0]}")
        report.append("")
    
    report.append("\n---\n")
    
    # === 3. æ–¹æ³•è«– ===
    report.append("## ğŸ”¬ ç ”ç©¶æ–¹æ³•è«–\n")
    report.append("### 3.1 è³‡æ–™ä¾†æºèˆ‡æ¨£æœ¬\n")
    report.append(f"æœ¬ç ”ç©¶åˆ†æ {len(df)} ç­†å•å·è³‡æ–™ï¼Œæ¶µè“‹ {len(cols_to_analyze)} å€‹åˆ†æé¢å‘ã€‚")
    
    if 'respondent_type' in df.columns:
        report.append("è³‡æ–™ä¾†æºåŒ…å«å…¬å¸æ–¹å¡«ç­”èˆ‡æŠ•è³‡æ–¹å¡«ç­”ï¼Œå¯é€²è¡Œé›™å‘æ¯”å°åˆ†æã€‚\n")
    
    report.append("### 3.2 çµ±è¨ˆåˆ†ææ–¹æ³•\n")
    report.append("æœ¬ç ”ç©¶æ¡ç”¨ä»¥ä¸‹çµ±è¨ˆæ–¹æ³•ï¼š\n")
    report.append("1. **æè¿°æ€§çµ±è¨ˆ**ï¼šè¨ˆç®—æ¬¡æ•¸åˆ†ä½ˆã€ç™¾åˆ†æ¯”ã€å¹³å‡æ•¸ã€ä¸­ä½æ•¸ç­‰åŸºæœ¬çµ±è¨ˆé‡")
    report.append("2. **å¡æ–¹æª¢å®šï¼ˆChi-square testï¼‰**ï¼šæª¢é©—é¡åˆ¥è®Šé …åœ¨ä¸åŒç¾¤é«”é–“çš„åˆ†ä½ˆå·®ç•°")
    report.append("3. **Mann-Whitney U æª¢å®š**ï¼šæª¢é©—æ•¸å€¼è®Šé …åœ¨å…©çµ„é–“çš„åˆ†ä½ˆå·®ç•°ï¼ˆéåƒæ•¸æª¢å®šï¼‰")
    report.append("4. **Kruskal-Wallis æª¢å®š**ï¼šæª¢é©—æ•¸å€¼è®Šé …åœ¨å¤šçµ„é–“çš„åˆ†ä½ˆå·®ç•°ï¼ˆéåƒæ•¸æª¢å®šï¼‰")
    report.append("5. **Fisher ç²¾ç¢ºæª¢å®š**ï¼šé‡å°å°æ¨£æœ¬çš„é¡åˆ¥è®Šé …é€²è¡Œç²¾ç¢ºæ©Ÿç‡æª¢å®š\n")
    
    report.append("### 3.3 é¡¯è‘—æ€§æ°´æº–\n")
    report.append("æœ¬ç ”ç©¶æ¡ç”¨ä»¥ä¸‹é¡¯è‘—æ€§æ¨™æº–ï¼š")
    report.append("- p < 0.001ï¼šæ¥µé¡¯è‘—å·®ç•° (â­â­â­)")
    report.append("- p < 0.01ï¼šéå¸¸é¡¯è‘—å·®ç•° (â­â­)")
    report.append("- p < 0.05ï¼šé¡¯è‘—å·®ç•° (â­)")
    report.append("- p â‰¥ 0.05ï¼šç„¡é¡¯è‘—å·®ç•°\n")
    
    report.append("\n---\n")
    
    # === 4. ä¸»è¦ç™¼ç¾ ===
    report.append("## ğŸ“Š ä¸»è¦ç™¼ç¾\n")
    
    # æŒ‰å„ªå…ˆé †åºåˆ†çµ„
    high_priority = [r for r in recommendations if r['å„ªå…ˆé †åº'] >= 3]
    medium_priority = [r for r in recommendations if 2 <= r['å„ªå…ˆé †åº'] < 3]
    
    if high_priority:
        report.append("### 4.1 é«˜åº¦é—œæ³¨è­°é¡Œï¼ˆå„ªå…ˆé †åº â‰¥ 3ï¼‰\n")
        report.append("ä»¥ä¸‹è­°é¡Œåœ¨çµ±è¨ˆåˆ†æä¸­å‘ˆç¾æ¥µé¡¯è‘—æˆ–å¤šé‡é¡¯è‘—å·®ç•°ï¼Œå»ºè­°å„ªå…ˆé—œæ³¨ï¼š\n")
        
        for idx, rec in enumerate(high_priority, 1):
            report.append(f"#### è­°é¡Œ {idx}ï¼š{rec['å®Œæ•´é¡Œç›®']}\n")
            report.append(f"**æ¨£æœ¬æ•¸ï¼š** {rec['æ¨£æœ¬æ•¸']} | **ç¼ºå¤±ç‡ï¼š** {rec['ç¼ºå¤±ç‡']} | **å„ªå…ˆé †åºï¼š** {rec['å„ªå…ˆé †åº']:.1f}\n")
            
            # çµ±è¨ˆçµæœè§£è®€
            if 'çµ±è¨ˆçµæœ' in rec and rec['çµ±è¨ˆçµæœ']:
                stats = rec['çµ±è¨ˆçµæœ']
                
                if 'p' in stats:
                    p_val = stats['p']
                    sig_level = "æ¥µé¡¯è‘—" if p_val < 0.001 else "éå¸¸é¡¯è‘—" if p_val < 0.01 else "é¡¯è‘—"
                    report.append(f"**çµ±è¨ˆæª¢å®šçµæœï¼š**")
                    report.append(f"- p-value = {p_val:.4f} ({sig_level})")
                    
                    if 'median_diff' in stats:
                        report.append(f"- ä¸­ä½æ•¸å·®ç•°ï¼š{stats['median_diff']:.2f}")
                    
                    # æ¥­å‹™è§£è®€
                    report.append(f"\n**æ¥­å‹™è§£è®€ï¼š**")
                    if p_val < 0.001:
                        report.append("æ­¤è­°é¡Œåœ¨ä¸åŒç¾¤é«”é–“å­˜åœ¨æ¥µé¡¯è‘—å·®ç•°ï¼ˆp < 0.001ï¼‰ï¼Œé¡¯ç¤ºé›™æ–¹åœ¨èªçŸ¥æˆ–å¯¦å‹™ä¸Šæœ‰æœ¬è³ªæ€§çš„å·®è·ã€‚å»ºè­°æ·±å…¥æ¢è¨é€ æˆå·®ç•°çš„çµæ§‹æ€§å› ç´ ï¼Œä¸¦è©•ä¼°æ˜¯å¦éœ€è¦æ”¿ç­–ä»‹å…¥æˆ–è¼”å°æ©Ÿåˆ¶ã€‚")
                    elif p_val < 0.01:
                        report.append("æ­¤è­°é¡Œå‘ˆç¾é«˜åº¦é¡¯è‘—å·®ç•°ï¼ˆp < 0.01ï¼‰ï¼Œåæ˜ ä¸åŒç¾¤é«”åœ¨æ­¤é¢å‘çš„ç¶“é©—æˆ–æœŸå¾…æœ‰æ˜é¡¯è½å·®ã€‚å»ºè­°ç´å…¥å¾ŒçºŒè¼”å°è¨ˆç•«çš„é‡é»é …ç›®ã€‚")
                    else:
                        report.append("æ­¤è­°é¡Œå­˜åœ¨é¡¯è‘—å·®ç•°ï¼ˆp < 0.05ï¼‰ï¼Œå€¼å¾—é—œæ³¨ä¸¦é€²ä¸€æ­¥åˆ†æå·®ç•°æˆå› ã€‚")
                
                if 'é¡¯è‘—é¸é …æ•¸' in stats:
                    sig_count = stats['é¡¯è‘—é¸é …æ•¸']
                    report.append(f"\n- æœ‰ {sig_count} å€‹é¸é …å‘ˆç¾é¡¯è‘—å·®ç•°")
                    report.append(f"- **è§£è®€ï¼š** æ­¤è¤‡é¸é¡Œä¸­æœ‰å¤šå€‹é¸é …åœ¨ä¸åŒç¾¤é«”é–“åˆ†ä½ˆä¸å‡ï¼Œé¡¯ç¤ºåœ¨å…·é«”å¯¦å‹™åšæ³•ä¸Šå­˜åœ¨ç³»çµ±æ€§å·®ç•°ã€‚")
            
            report.append("\n" + "- " * 30 + "\n")
    
    if medium_priority:
        report.append("\n### 4.2 é‡è¦è­°é¡Œï¼ˆå„ªå…ˆé †åº 2-3ï¼‰\n")
        report.append("ä»¥ä¸‹è­°é¡Œå…·æœ‰çµ±è¨ˆé¡¯è‘—æ€§æˆ–é«˜è³‡æ–™å®Œæ•´åº¦ï¼Œå€¼å¾—ç´å…¥å ±å‘Šï¼š\n")
        
        for idx, rec in enumerate(medium_priority, 1):
            report.append(f"**{idx}. {rec['å®Œæ•´é¡Œç›®'][:80]}{'...' if len(rec['å®Œæ•´é¡Œç›®']) > 80 else ''}**")
            report.append(f"- æ¨£æœ¬æ•¸ï¼š{rec['æ¨£æœ¬æ•¸']} | ç¼ºå¤±ç‡ï¼š{rec['ç¼ºå¤±ç‡']}")
            report.append(f"- é‡é»ï¼š{'; '.join(rec['æ¨è–¦ç†ç”±'][:2])}")
            report.append("")
    
    report.append("\n---\n")
    
    # === 5. çµè«–èˆ‡å»ºè­° ===
    report.append("## ğŸ’¡ çµè«–èˆ‡æ”¿ç­–å»ºè­°\n")
    
    report.append("### 5.1 ç¸½é«”è§€å¯Ÿ\n")
    report.append(f"æœ¬æ¬¡å•å·åˆ†ææ¶µè“‹ {len(recommendations)} å€‹å…·æœ‰åˆ†æåƒ¹å€¼çš„è­°é¡Œï¼Œ")
    report.append(f"å…¶ä¸­ {len(high_priority)} å€‹è­°é¡Œå‘ˆç¾é«˜åº¦é¡¯è‘—å·®ç•°ï¼Œ{len(medium_priority)} å€‹è­°é¡Œå…·æœ‰é‡è¦åƒè€ƒåƒ¹å€¼ã€‚\n")
    
    if 'respondent_type' in df.columns:
        report.append("### 5.2 å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹çš„èªçŸ¥è½å·®\n")
        report.append("åˆ†æé¡¯ç¤ºå…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹åœ¨å¤šé …å…¬å¸æ²»ç†è­°é¡Œä¸Šå­˜åœ¨èªçŸ¥æˆ–å¯¦å‹™å·®ç•°ã€‚")
        report.append("æ­¤è½å·®å¯èƒ½ä¾†è‡ªæ–¼ï¼š")
        report.append("- **è³‡è¨Šä¸å°ç¨±**ï¼šæŠ•è³‡æ–¹å°å…¬å¸å¯¦å‹™çš„äº†è§£ç¨‹åº¦æœ‰é™")
        report.append("- **æœŸå¾…å·®ç•°**ï¼šé›™æ–¹å°æ²»ç†æ¨™æº–çš„èªçŸ¥ä¸ä¸€è‡´")
        report.append("- **å¯¦å‹™è½å·®**ï¼šå…¬å¸è‡ªè©•èˆ‡å¤–éƒ¨è©•ä¼°çš„å®¢è§€æ€§å·®ç•°\n")
    
    report.append("### 5.3 æ”¿ç­–å»ºè­°\n")
    report.append("åŸºæ–¼ä¸Šè¿°åˆ†æçµæœï¼Œæœ¬ç ”ç©¶æå‡ºä»¥ä¸‹æ”¿ç­–å»ºè­°ä¾›åœ‹ç™¼åŸºé‡‘åƒè€ƒï¼š\n")
    
    # æ ¹æ“šé«˜å„ªå…ˆé †åºè­°é¡Œç”Ÿæˆå…·é«”å»ºè­°
    if high_priority:
        report.append("**é‡å°é«˜åº¦é—œæ³¨è­°é¡Œï¼š**\n")
        
        # åˆ†ææ˜¯å¦æœ‰ç‰¹å®šé ˜åŸŸçš„å•é¡Œ
        governance_issues = [r for r in high_priority if any(kw in r['å®Œæ•´é¡Œç›®'] for kw in ['è‘£äº‹æœƒ', 'è‘£äº‹', 'ç›£å¯Ÿäºº'])]
        transparency_issues = [r for r in high_priority if any(kw in r['å®Œæ•´é¡Œç›®'] for kw in ['æ­éœ²', 'é€æ˜', 'è³‡è¨Š'])]
        internal_control_issues = [r for r in high_priority if any(kw in r['å®Œæ•´é¡Œç›®'] for kw in ['å…§éƒ¨æ§åˆ¶', 'æµç¨‹', 'åˆ¶åº¦'])]
        
        if governance_issues:
            report.append("1. **å¼·åŒ–è‘£äº‹æœƒé‹ä½œæ©Ÿåˆ¶**")
            report.append("   - å»ºè­°æä¾›æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†è¨“ç·´èª²ç¨‹")
            report.append("   - æ¨å‹•ç¨ç«‹è‘£äº‹æˆ–å¤–éƒ¨è‘£äº‹åˆ¶åº¦")
            report.append("   - å»ºç«‹è‘£äº‹æœƒé‹ä½œè©•ä¼°æ©Ÿåˆ¶\n")
        
        if transparency_issues:
            report.append("2. **æå‡è³‡è¨Šé€æ˜åº¦**")
            report.append("   - å»ºç«‹è³‡è¨Šæ­éœ²æ¨™æº–ç¯„æœ¬")
            report.append("   - é¼“å‹µå®šæœŸå‘è‚¡æ±å ±å‘Š")
            report.append("   - æ¨å‹•æ•¸ä½åŒ–è³‡è¨Šå¹³å°\n")
        
        if internal_control_issues:
            report.append("3. **å»ºç«‹å…§éƒ¨æ§åˆ¶åˆ¶åº¦**")
            report.append("   - æä¾›å…§æ§å»ºç½®è¼”å°æœå‹™")
            report.append("   - åˆ†äº«æœ€ä½³å¯¦å‹™æ¡ˆä¾‹")
            report.append("   - å»ºç«‹åˆ†éšæ®µå°å…¥æ©Ÿåˆ¶\n")
    
    report.append("4. **ç¸®å°å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹èªçŸ¥è½å·®**")
    report.append("   - å®šæœŸèˆ‰è¾¦æºé€šåº§è«‡æœƒ")
    report.append("   - å»ºç«‹é›™å‘å›é¥‹æ©Ÿåˆ¶")
    report.append("   - æä¾›ç¬¬ä¸‰æ–¹æ²»ç†è©•ä¼°æœå‹™\n")
    
    report.append("5. **éšæ®µæ€§è¼”å°æ©Ÿåˆ¶**")
    report.append("   - é‡å°ä¸åŒç™¼å±•éšæ®µæä¾›å®¢è£½åŒ–è¼”å°")
    report.append("   - å»ºç«‹æ¨™ç«¿ä¼æ¥­ç¤ºç¯„æ¡ˆä¾‹")
    report.append("   - æä¾›æŒçºŒè¿½è¹¤èˆ‡è©•ä¼°\n")
    
    report.append("\n---\n")
    
    # === 6. é™„éŒ„ ===
    report.append("## ğŸ“ é™„éŒ„\n")
    report.append("### é™„éŒ„ Aï¼šå®Œæ•´åˆ†æè­°é¡Œæ¸…å–®\n")
    report.append(f"æœ¬æ¬¡åˆ†æå…±æ¶µè“‹ {len(recommendations)} å€‹è­°é¡Œï¼Œå®Œæ•´æ¸…å–®å¦‚ä¸‹ï¼š\n")
    
    report.append("| æ’å | é¡Œç›® | æ¨£æœ¬æ•¸ | ç¼ºå¤±ç‡ | å„ªå…ˆé †åº |")
    report.append("|------|------|--------|--------|----------|")
    
    for idx, rec in enumerate(recommendations[:20], 1):  # åªé¡¯ç¤ºå‰20é¡Œ
        topic_short = rec['é¡Œç›®'][:40] + '...' if len(rec['é¡Œç›®']) > 40 else rec['é¡Œç›®']
        report.append(f"| {idx} | {topic_short} | {rec['æ¨£æœ¬æ•¸']} | {rec['ç¼ºå¤±ç‡']} | {rec['å„ªå…ˆé †åº']:.1f} |")
    
    if len(recommendations) > 20:
        report.append(f"\n*è¨»ï¼šå®Œæ•´æ¸…å–®åŒ…å« {len(recommendations)} å€‹è­°é¡Œï¼Œæ­¤è™•åƒ…é¡¯ç¤ºå‰ 20 é¡Œ*\n")
    
    report.append("\n### é™„éŒ„ Bï¼šçµ±è¨ˆæ–¹æ³•èªªæ˜\n")
    report.append("**å¡æ–¹æª¢å®šï¼ˆChi-square testï¼‰**")
    report.append("- é©ç”¨æ–¼é¡åˆ¥è®Šé …çš„ç¨ç«‹æ€§æª¢å®š")
    report.append("- é›¶å‡è¨­ï¼šå…©å€‹é¡åˆ¥è®Šé …ä¹‹é–“ç¨ç«‹ï¼ˆç„¡é—œè¯ï¼‰")
    report.append("- ç•¶ p < 0.05 æ™‚æ‹’çµ•é›¶å‡è¨­ï¼Œèªç‚ºè®Šé …é–“å­˜åœ¨é—œè¯\n")
    
    report.append("**Mann-Whitney U æª¢å®š**")
    report.append("- éåƒæ•¸æª¢å®šæ–¹æ³•ï¼Œä¸å‡è¨­è³‡æ–™ç¬¦åˆå¸¸æ…‹åˆ†ä½ˆ")
    report.append("- é©ç”¨æ–¼æ¯”è¼ƒå…©çµ„ç¨ç«‹æ¨£æœ¬çš„åˆ†ä½ˆ")
    report.append("- æª¢é©—å…©çµ„çš„ä¸­ä½æ•¸æ˜¯å¦æœ‰é¡¯è‘—å·®ç•°\n")
    
    report.append("**Kruskal-Wallis æª¢å®š**")
    report.append("- Mann-Whitney U æª¢å®šçš„æ“´å±•ç‰ˆæœ¬")
    report.append("- é©ç”¨æ–¼æ¯”è¼ƒä¸‰çµ„æˆ–ä»¥ä¸Šç¨ç«‹æ¨£æœ¬")
    report.append("- æª¢é©—å¤šçµ„é–“æ˜¯å¦å­˜åœ¨é¡¯è‘—å·®ç•°\n")
    
    report.append("\n---\n")
    report.append(f"\n**å ±å‘ŠçµæŸ** | ç”¢ç”Ÿæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    return "\n".join(report)

# åŸ·è¡Œé¡Œç›®åˆä½µ
st.markdown("### ğŸ”„ æ­£åœ¨é€²è¡Œé¡Œç›®å»é‡èˆ‡åˆä½µ...")
with st.spinner("åˆ†æé¡Œç›®ç›¸ä¼¼åº¦ä¸­..."):
    if analysis_mode == 'åˆä½µåˆ†æ':
        merged_mapping, cols_to_analyze = merge_similar_questions(
            df_to_analyze, 
            cols_to_exclude, 
            similarity_threshold=0.70  # é™ä½é–¾å€¼ï¼Œæ›´ç©æ¥µåˆä½µ
        )
    else:
        # é€é¡Œç€è¦½æ¨¡å¼ï¼šä¸åˆä½µï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ¬„ä½
        cols_to_analyze = [c for c in df_to_analyze.columns if c not in cols_to_exclude]
        merged_mapping = {c: [c] for c in cols_to_analyze}  # å»ºç«‹ä¸€å°ä¸€æ˜ å°„

# é¡¯ç¤ºåˆä½µçµæœï¼ˆåªåœ¨åˆä½µåˆ†ææ¨¡å¼ä¸‹é¡¯ç¤ºï¼‰
if analysis_mode == 'åˆä½µåˆ†æ':
    with st.expander("ğŸ” é¡Œç›®åˆä½µè©³ç´°è³‡è¨Šï¼ˆé™¤éŒ¯ç”¨ï¼‰", expanded=False):
        duplicate_groups = {k: v for k, v in merged_mapping.items() if len(v) > 1}
        
        if duplicate_groups:
            st.success(f"âœ… æˆåŠŸåˆä½µ {len(duplicate_groups)} çµ„é‡è¤‡é¡Œç›®ï¼Œå…±æ¸›å°‘ {sum(len(v)-1 for v in duplicate_groups.values())} å€‹é‡è¤‡é …")
            
            # çµ±è¨ˆåˆä½µæ•ˆæœ
            if 'respondent_type' in df_to_analyze.columns:
                company_only = 0
                investor_only = 0
                mixed = 0
                
                for representative, originals in duplicate_groups.items():
                    respondent_types = set()
                    for orig in originals:
                        data = df_to_analyze[orig].dropna()
                        if not data.empty:
                            types = df_to_analyze.loc[data.index, 'respondent_type'].unique()
                            respondent_types.update(types)
                    
                    if 'å…¬å¸æ–¹' in respondent_types and 'æŠ•è³‡æ–¹' in respondent_types:
                        mixed += 1
                    elif 'å…¬å¸æ–¹' in respondent_types:
                        company_only += 1
                    elif 'æŠ•è³‡æ–¹' in respondent_types:
                        investor_only += 1
                
                st.write(f"- ğŸ”µ å…¬å¸æ–¹å°ˆç”¨é¡Œç›®åˆä½µï¼š{company_only} çµ„")
                st.write(f"- ğŸŸ  æŠ•è³‡æ–¹å°ˆç”¨é¡Œç›®åˆä½µï¼š{investor_only} çµ„")
                st.write(f"- ğŸŸ¢ è·¨èº«åˆ†é¡Œç›®åˆä½µï¼š{mixed} çµ„")
            
            # é¡¯ç¤ºç¯„ä¾‹ï¼ˆå‰ 10 çµ„ï¼‰
            st.markdown("**åˆä½µç¯„ä¾‹ï¼ˆå‰ 10 çµ„ï¼‰ï¼š**")
            for i, (representative, originals) in enumerate(list(duplicate_groups.items())[:10], 1):
                st.markdown(f"**{i}. ä»£è¡¨é¡Œç›®ï¼š** {representative}")
                normalized_rep = normalize_question_v2(representative)
                st.caption(f"æ¨™æº–åŒ–ç‚ºï¼š{normalized_rep}")
                
                for orig in originals:
                    if orig == representative:
                        continue
                    similarity = calculate_similarity(
                        normalize_question_v2(representative), 
                        normalize_question_v2(orig)
                    )
                    orig_data = df_to_analyze[orig].dropna()
                    if not orig_data.empty and 'respondent_type' in df_to_analyze.columns:
                        respondents = df_to_analyze.loc[orig_data.index, 'respondent_type'].value_counts().to_dict()
                        resp_str = ", ".join([f"{k}:{v}ç­†" for k, v in respondents.items()])
                        st.write(f"  â†³ {orig}")
                        st.caption(f"    ç›¸ä¼¼åº¦: {similarity:.2%} | è³‡æ–™: {resp_str}")
                    else:
                        st.write(f"  â†³ {orig} (ç„¡è³‡æ–™)")
                
                st.markdown("---")
            
            if len(duplicate_groups) > 10:
                st.info(f"é‚„æœ‰ {len(duplicate_groups)-10} çµ„åˆä½µé¡Œç›®æœªé¡¯ç¤º...")
        else:
            st.success("âœ… æ²’æœ‰ç™¼ç¾éœ€è¦åˆä½µçš„é‡è¤‡é¡Œç›®")
        
        st.metric("æœ€çµ‚åˆ†æé¡Œç›®æ•¸", len(cols_to_analyze), 
                  delta=f"-{len(df_to_analyze.columns) - len(cols_to_exclude) - len(cols_to_analyze)}" if len(df_to_analyze.columns) - len(cols_to_exclude) > len(cols_to_analyze) else "0")

# --- åŠŸèƒ½å€ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰---
st.markdown("---")

# ç”Ÿæˆå ±å‘Šæ¨è–¦
if analysis_mode == 'åˆä½µåˆ†æ':
    st.markdown("---")
    st.subheader("ğŸ“‹ é©åˆå¯«å…¥å ±å‘Šçš„é¡Œç›®æ¨è–¦")
    
    with st.spinner("æ­£åœ¨åˆ†æä¸¦æ¨è–¦é‡è¦é¡Œç›®..."):
        recommendations = generate_report_recommendations(df_to_analyze, cols_to_analyze, analysis_mode)
    
    if recommendations:
        st.success(f"âœ… æ‰¾åˆ° {len(recommendations)} é¡Œå…·æœ‰åˆ†æåƒ¹å€¼çš„é¡Œç›®")
        
        # é¡¯ç¤ºå‰ 20 é¡Œæ¨è–¦
        rec_df = pd.DataFrame([{
            'æ’å': i+1,
            'é¡Œç›®': rec['é¡Œç›®'],
            'æ¨£æœ¬æ•¸': rec['æ¨£æœ¬æ•¸'],
            'ç¼ºå¤±ç‡': rec['ç¼ºå¤±ç‡'],
            'æ¨è–¦ç†ç”±': 'ï¼›'.join(rec['æ¨è–¦ç†ç”±']),
            'å„ªå…ˆé †åºåˆ†æ•¸': f"{rec['å„ªå…ˆé †åº']:.1f}"
        } for i, rec in enumerate(recommendations[:20])])
        
        st.info("ğŸ’¡ **ä½¿ç”¨å»ºè­°**ï¼šå„ªå…ˆé †åºåˆ†æ•¸ â‰¥ 2 çš„é¡Œç›®é€šå¸¸å…·æœ‰è¼ƒé«˜çš„å ±å‘Šåƒ¹å€¼")
        st.dataframe(rec_df, use_container_width=True)
        
        # === æ–°å¢ï¼šæ·±åº¦åˆ†æå ±å‘Š ===
        st.markdown("---")
        st.markdown("### ğŸ“Š æ·±åº¦åˆ†æå ±å‘Š")
        
        # è®“ä½¿ç”¨è€…é¸æ“‡è¦æ·±å…¥åˆ†æçš„é¡Œç›®
        high_priority_recs = [rec for rec in recommendations if rec['å„ªå…ˆé †åº'] >= 2]
        if high_priority_recs:
            selected_topics = st.multiselect(
                "é¸æ“‡è¦æ·±å…¥åˆ†æçš„é¡Œç›®ï¼ˆé è¨­ç‚ºå„ªå…ˆé †åº â‰¥ 2 çš„é¡Œç›®ï¼‰:",
                options=[rec['å®Œæ•´é¡Œç›®'] for rec in high_priority_recs],
                default=[rec['å®Œæ•´é¡Œç›®'] for rec in high_priority_recs[:5]]  # é è¨­å‰5é¡Œ
            )
            
            if selected_topics:
                for topic in selected_topics:
                    # æ‰¾åˆ°å°æ‡‰çš„æ¨è–¦è³‡è¨Š
                    rec_info = next((r for r in recommendations if r['å®Œæ•´é¡Œç›®'] == topic), None)
                    if not rec_info:
                        continue
                    
                    with st.expander(f"ğŸ“ˆ {rec_info['é¡Œç›®']}", expanded=False):
                        col_data = df_to_analyze[topic].dropna()
                        if col_data.empty:
                            st.warning("ç„¡æœ‰æ•ˆè³‡æ–™")
                            continue
                        
                        # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
                        st.markdown("#### ğŸ“‹ åŸºæœ¬è³‡è¨Š")
                        info_cols = st.columns(3)
                        info_cols[0].metric("æ¨£æœ¬æ•¸", rec_info['æ¨£æœ¬æ•¸'])
                        info_cols[1].metric("ç¼ºå¤±ç‡", rec_info['ç¼ºå¤±ç‡'])
                        info_cols[2].metric("å„ªå…ˆé †åº", f"{rec_info['å„ªå…ˆé †åº']:.1f}")
                        
                        st.markdown("**æ¨è–¦ç†ç”±ï¼š**")
                        for reason in rec_info['æ¨è–¦ç†ç”±']:
                            st.write(f"- {reason}")
                        
                        # åˆ¤æ–·é¡Œå‹
                        is_multiselect = col_data.dtype == 'object' and col_data.astype(str).str.contains('\n', na=False).any()
                        is_numeric = pd.api.types.is_numeric_dtype(col_data)
                        
                        # çµ±ä¸€è™•ç†æ•¸å€¼è³‡æ–™
                        col_data_numeric = None
                        if is_numeric:
                            col_data_numeric = pd.to_numeric(col_data, errors='coerce').dropna()
                        else:
                            numeric_version = pd.to_numeric(col_data, errors='coerce').dropna()
                            if len(numeric_version) > 0 and (len(numeric_version) / len(col_data) > 0.7):
                                is_numeric = True
                                col_data_numeric = numeric_version
                        
                        # === åˆ†æ1: å…¬å¸æ–¹ vs æŠ•è³‡æ–¹ ===
                        if 'respondent_type' in df_to_analyze.columns:
                            st.markdown("---")
                            st.markdown("#### ğŸ”µğŸŸ  å…¬å¸æ–¹ vs æŠ•è³‡æ–¹æ¯”è¼ƒ")
                            
                            if is_multiselect:
                                # è¤‡é¸é¡Œåˆ†æ
                                exploded = col_data.astype(str).str.split('\n').explode().str.strip()
                                exploded = exploded[(exploded != '') & (exploded != 'nan') & exploded.notna()]
                                
                                if not exploded.empty:
                                    df_exp = exploded.to_frame(name='option')
                                    df_exp['respondent_type'] = df_to_analyze.loc[df_exp.index, 'respondent_type'].fillna('æœªçŸ¥')
                                    
                                    # è¨ˆç®—å„é¸é …åœ¨ä¸åŒèº«åˆ†çš„æ¯”ä¾‹
                                    crosstab = pd.crosstab(df_exp['option'], df_exp['respondent_type'], normalize='columns') * 100
                                    
                                    # æ™ºæ…§æ’åº x è»¸
                                    sorted_index = smart_sort_categories(crosstab.index)
                                    crosstab = crosstab.reindex(sorted_index)
                                    
                                    if crosstab.shape[1] >= 2:
                                        # ç¹ªè£½å †ç–Šé•·æ¢åœ–
                                        fig = go.Figure()
                                        colors = {'å…¬å¸æ–¹': '#1f77b4', 'æŠ•è³‡æ–¹': '#ff7f0e', 'æœªçŸ¥': '#999999'}
                                        
                                        for resp_type in crosstab.columns:
                                            fig.add_trace(go.Bar(
                                                name=resp_type,
                                                x=crosstab.index,
                                                y=crosstab[resp_type],
                                                marker_color=colors.get(resp_type, '#cccccc'),
                                                text=[f"{v:.1f}%" for v in crosstab[resp_type]],
                                                textposition='auto'
                                            ))
                                        
                                        fig.update_layout(
                                            barmode='group',
                                            title='å„é¸é …åœ¨ä¸åŒèº«åˆ†çš„é¸æ“‡æ¯”ä¾‹',
                                            xaxis_title='é¸é …',
                                            yaxis_title='æ¯”ä¾‹ (%)',
                                            template='plotly_white',
                                            height=500,
                                            xaxis_tickangle=-45,
                                            xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                        
                                        # é¡¯è‘—å·®ç•°çš„é¸é …
                                        if 'é¡¯è‘—é¸é …' in rec_info['çµ±è¨ˆçµæœ']:
                                            st.markdown("**çµ±è¨ˆæª¢å®šçµæœï¼ˆå¡æ–¹æª¢å®šï¼‰ï¼š**")
                                            for sig_opt in rec_info['çµ±è¨ˆçµæœ']['é¡¯è‘—é¸é …'][:5]:
                                                p_val = sig_opt['p']
                                                significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*"
                                                st.write(f"- é¸é …ã€Œ{sig_opt['é¸é …']}ã€ï¼šå…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹é¸æ“‡æ¯”ä¾‹æœ‰é¡¯è‘—å·®ç•° (p = {p_val:.4f} {significance})")
                            
                            elif is_numeric:
                                # æ•¸å€¼é¡Œåˆ†æ
                                df_numeric = col_data_numeric.to_frame(name='value')
                                df_numeric['respondent_type'] = df_to_analyze.loc[df_numeric.index, 'respondent_type'].fillna('æœªçŸ¥')
                                
                                # ç¹ªè£½ç›’ç‹€åœ–
                                fig = go.Figure()
                                colors = {'å…¬å¸æ–¹': '#1f77b4', 'æŠ•è³‡æ–¹': '#ff7f0e', 'æœªçŸ¥': '#999999'}
                                
                                for resp_type in df_numeric['respondent_type'].unique():
                                    data_subset = df_numeric[df_numeric['respondent_type'] == resp_type]['value']
                                    fig.add_trace(go.Box(
                                        y=data_subset,
                                        name=resp_type,
                                        marker_color=colors.get(resp_type, '#cccccc'),
                                        boxmean='sd'
                                    ))
                                
                                fig.update_layout(
                                    title='æ•¸å€¼åˆ†ä½ˆæ¯”è¼ƒ',
                                    yaxis_title='æ•¸å€¼',
                                    template='plotly_white',
                                    height=400
                                )
                                st.plotly_chart(fig, use_container_width=True)
                                
                                # çµ±è¨ˆæ‘˜è¦è¡¨
                                summary = df_numeric.groupby('respondent_type')['value'].describe()
                                st.dataframe(summary.style.format("{:.2f}"), use_container_width=True)
                                
                                # Mann-Whitney U æª¢å®š
                                if 'p' in rec_info['çµ±è¨ˆçµæœ']:
                                    p_val = rec_info['çµ±è¨ˆçµæœ']['p']
                                    median_diff = rec_info['çµ±è¨ˆçµæœ'].get('median_diff', 0)
                                    significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*"
                                    
                                    st.markdown("**çµ±è¨ˆæª¢å®šçµæœï¼ˆMann-Whitney U æª¢å®šï¼‰ï¼š**")
                                    st.write(f"- p-value = {p_val:.4f} {significance}")
                                    st.write(f"- ä¸­ä½æ•¸å·®ç•° = {median_diff:.2f}")
                                    
                                    if p_val < 0.05:
                                        st.success("âœ… å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹çš„æ•¸å€¼åˆ†ä½ˆæœ‰é¡¯è‘—å·®ç•°")
                                    else:
                                        st.info("â„¹ï¸ å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹çš„æ•¸å€¼åˆ†ä½ˆç„¡é¡¯è‘—å·®ç•°")
                            
                            else:
                                # é¡åˆ¥é¡Œåˆ†æ
                                s = col_data.astype(str)
                                s = s[~s.str.lower().str.contains('nan', na=False)]
                                
                                if not s.empty:
                                    df_cat = s.to_frame(name='category')
                                    df_cat['respondent_type'] = df_to_analyze.loc[df_cat.index, 'respondent_type'].fillna('æœªçŸ¥')
                                    
                                    # è¨ˆç®—æ¯”ä¾‹
                                    crosstab = pd.crosstab(df_cat['category'], df_cat['respondent_type'], normalize='columns') * 100
                                    
                                    # æ™ºæ…§æ’åº x è»¸
                                    sorted_index = smart_sort_categories(crosstab.index)
                                    crosstab = crosstab.reindex(sorted_index)
                                    
                                    if crosstab.shape[1] >= 2:
                                        # ç¹ªè£½åˆ†çµ„é•·æ¢åœ–
                                        fig = go.Figure()
                                        colors = {'å…¬å¸æ–¹': '#1f77b4', 'æŠ•è³‡æ–¹': '#ff7f0e', 'æœªçŸ¥': '#999999'}
                                        
                                        for resp_type in crosstab.columns:
                                            fig.add_trace(go.Bar(
                                                name=resp_type,
                                                x=crosstab.index,
                                                y=crosstab[resp_type],
                                                marker_color=colors.get(resp_type, '#cccccc'),
                                                text=[f"{v:.1f}%" for v in crosstab[resp_type]],
                                                textposition='auto'
                                            ))
                                        
                                        fig.update_layout(
                                            barmode='group',
                                            title='å„é¡åˆ¥åœ¨ä¸åŒèº«åˆ†çš„åˆ†ä½ˆæ¯”ä¾‹',
                                            xaxis_title='é¡åˆ¥',
                                            yaxis_title='æ¯”ä¾‹ (%)',
                                            template='plotly_white',
                                            height=400,
                                            xaxis_tickangle=-45,
                                            xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                        
                                        # çµ±è¨ˆæª¢å®š
                                        if 'p' in rec_info['çµ±è¨ˆçµæœ']:
                                            p_val = rec_info['çµ±è¨ˆçµæœ']['p']
                                            significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*"
                                            
                                            st.markdown("**çµ±è¨ˆæª¢å®šçµæœï¼ˆå¡æ–¹æª¢å®š/Fisherç²¾ç¢ºæª¢å®šï¼‰ï¼š**")
                                            st.write(f"- p-value = {p_val:.4f} {significance}")
                                            
                                            if p_val < 0.05:
                                                st.success("âœ… å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹çš„åˆ†ä½ˆæœ‰é¡¯è‘—å·®ç•°")
                                            else:
                                                st.info("â„¹ï¸ å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹çš„åˆ†ä½ˆç„¡é¡¯è‘—å·®ç•°")
                        
                        # === åˆ†æ2: éšæ®µæ¯”è¼ƒ (ä¸€éšæ®µ vs äºŒéšæ®µ vs ä¸‰éšæ®µ) ===
                        if PHASE_COLUMN_NAME in df_to_analyze.columns and df_to_analyze[PHASE_COLUMN_NAME].notna().any():
                            phase_nunique = df_to_analyze.loc[col_data.index, PHASE_COLUMN_NAME].nunique()
                            
                            if phase_nunique > 1:
                                st.markdown("---")
                                st.markdown("#### ğŸ”¢ éšæ®µæ¯”è¼ƒåˆ†æï¼ˆä¸€éšæ®µ vs äºŒéšæ®µ vs ä¸‰éšæ®µï¼‰")
                                
                                if is_multiselect:
                                    # è¤‡é¸é¡Œéšæ®µåˆ†æ
                                    exploded = col_data.astype(str).str.split('\n').explode().str.strip()
                                    exploded = exploded[(exploded != '') & (exploded != 'nan') & exploded.notna()]
                                    
                                    if not exploded.empty:
                                        df_exp = exploded.to_frame(name='option')
                                        df_exp['phase'] = df_to_analyze.loc[df_exp.index, PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»')
                                        
                                        # è¨ˆç®—å„é¸é …åœ¨ä¸åŒéšæ®µçš„æ¯”ä¾‹
                                        crosstab_phase = pd.crosstab(df_exp['option'], df_exp['phase'], normalize='columns') * 100
                                        
                                        # æ™ºæ…§æ’åº x è»¸
                                        sorted_index = smart_sort_categories(crosstab_phase.index)
                                        crosstab_phase = crosstab_phase.reindex(sorted_index)
                                        
                                        # ç¹ªè£½å †ç–Šé•·æ¢åœ–
                                        fig = go.Figure()
                                        colors = ['#2ca02c', '#d62728', '#9467bd', '#8c564b']
                                        
                                        for idx, phase in enumerate(sorted(crosstab_phase.columns)):
                                            fig.add_trace(go.Bar(
                                                name=str(phase),
                                                x=crosstab_phase.index,
                                                y=crosstab_phase[phase],
                                                marker_color=colors[idx % len(colors)],
                                                text=[f"{v:.1f}%" for v in crosstab_phase[phase]],
                                                textposition='auto'
                                            ))
                                        
                                        fig.update_layout(
                                            barmode='group',
                                            title='å„é¸é …åœ¨ä¸åŒéšæ®µçš„é¸æ“‡æ¯”ä¾‹',
                                            xaxis_title='é¸é …',
                                            yaxis_title='æ¯”ä¾‹ (%)',
                                            template='plotly_white',
                                            height=500,
                                            xaxis_tickangle=-45,
                                            xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                        
                                        # å¡æ–¹æª¢å®šï¼ˆæª¢æŸ¥å„é¸é …åœ¨éšæ®µé–“æ˜¯å¦æœ‰å·®ç•°ï¼‰
                                        st.markdown("**çµ±è¨ˆæª¢å®šçµæœï¼ˆå¡æ–¹æª¢å®šï¼‰ï¼š**")
                                        significant_options = []
                                        
                                        for opt in df_exp['option'].unique()[:10]:
                                            if pd.isna(opt):
                                                continue
                                            pres = df_to_analyze[topic].astype(str).fillna('').apply(
                                                lambda s: opt in [x.strip() for x in s.split('\n') if x.strip()]
                                            )
                                            table = pd.crosstab(pres, df_to_analyze.loc[pres.index, PHASE_COLUMN_NAME])
                                            
                                            if table.size > 0 and table.values.sum() > 0 and table.shape[0] >= 2 and table.shape[1] >= 2:
                                                try:
                                                    chi2, p, dof, exp = chi2_contingency(table)
                                                    if np.nanmin(exp) > 1 and p < 0.05:
                                                        significance = "***" if p < 0.001 else "**" if p < 0.01 else "*"
                                                        significant_options.append((opt, p, significance))
                                                except:
                                                    pass
                                        
                                        if significant_options:
                                            for opt, p, sig in significant_options[:5]:
                                                st.write(f"- é¸é …ã€Œ{opt}ã€ï¼šä¸åŒéšæ®µé–“æœ‰é¡¯è‘—å·®ç•° (p = {p:.4f} {sig})")
                                        else:
                                            st.info("â„¹ï¸ å„é¸é …åœ¨ä¸åŒéšæ®µé–“ç„¡é¡¯è‘—å·®ç•°")
                                
                                elif is_numeric:
                                    # æ•¸å€¼é¡Œéšæ®µåˆ†æ
                                    df_numeric_phase = col_data_numeric.to_frame(name='value')
                                    df_numeric_phase['phase'] = df_to_analyze.loc[df_numeric_phase.index, PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»')
                                    
                                    # ç¹ªè£½ç›’ç‹€åœ–
                                    fig = go.Figure()
                                    colors = ['#2ca02c', '#d62728', '#9467bd', '#8c564b']
                                    
                                    for idx, phase in enumerate(sorted(df_numeric_phase['phase'].unique())):
                                        data_subset = df_numeric_phase[df_numeric_phase['phase'] == phase]['value']
                                        fig.add_trace(go.Box(
                                            y=data_subset,
                                            name=str(phase),
                                            marker_color=colors[idx % len(colors)],
                                            boxmean='sd'
                                        ))
                                    
                                    fig.update_layout(
                                        title='ä¸åŒéšæ®µçš„æ•¸å€¼åˆ†ä½ˆæ¯”è¼ƒ',
                                        yaxis_title='æ•¸å€¼',
                                        template='plotly_white',
                                        height=400
                                    )
                                    st.plotly_chart(fig, use_container_width=True)
                                    
                                    # çµ±è¨ˆæ‘˜è¦è¡¨
                                    summary_phase = df_numeric_phase.groupby('phase')['value'].describe()
                                    st.dataframe(summary_phase.style.format("{:.2f}"), use_container_width=True)
                                    
                                    # Kruskal-Wallis æª¢å®š
                                    phases = df_numeric_phase['phase'].unique()
                                    if len(phases) >= 2:
                                        groups = [df_numeric_phase[df_numeric_phase['phase'] == p]['value'].values for p in phases]
                                        groups = [g for g in groups if len(g) > 0]
                                        
                                        if len(groups) >= 2:
                                            try:
                                                if len(groups) == 2:
                                                    stat, p_val = mannwhitneyu(groups[0], groups[1], alternative='two-sided')
                                                    test_name = "Mann-Whitney U æª¢å®š"
                                                else:
                                                    stat, p_val = kruskal(*groups)
                                                    test_name = "Kruskal-Wallis æª¢å®š"
                                                
                                                significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*"
                                                
                                                st.markdown(f"**çµ±è¨ˆæª¢å®šçµæœï¼ˆ{test_name}ï¼‰ï¼š**")
                                                st.write(f"- p-value = {p_val:.4f} {significance}")
                                                
                                                if p_val < 0.05:
                                                    st.success("âœ… ä¸åŒéšæ®µçš„æ•¸å€¼åˆ†ä½ˆæœ‰é¡¯è‘—å·®ç•°")
                                                else:
                                                    st.info("â„¹ï¸ ä¸åŒéšæ®µçš„æ•¸å€¼åˆ†ä½ˆç„¡é¡¯è‘—å·®ç•°")
                                            except Exception as e:
                                                st.warning(f"ç„¡æ³•é€²è¡Œçµ±è¨ˆæª¢å®šï¼š{str(e)}")
                                
                                else:
                                    # é¡åˆ¥é¡Œéšæ®µåˆ†æ
                                    s = col_data.astype(str)
                                    s = s[~s.str.lower().str.contains('nan', na=False)]
                                    
                                    if not s.empty:
                                        df_cat_phase = s.to_frame(name='category')
                                        df_cat_phase['phase'] = df_to_analyze.loc[df_cat_phase.index, PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»')
                                        
                                        # è¨ˆç®—æ¯”ä¾‹
                                        crosstab_phase = pd.crosstab(df_cat_phase['category'], df_cat_phase['phase'], normalize='columns') * 100
                                        
                                        # æ™ºæ…§æ’åº x è»¸
                                        sorted_index = smart_sort_categories(crosstab_phase.index)
                                        crosstab_phase = crosstab_phase.reindex(sorted_index)
                                        
                                        # ç¹ªè£½åˆ†çµ„é•·æ¢åœ–
                                        fig = go.Figure()
                                        colors = ['#2ca02c', '#d62728', '#9467bd', '#8c564b']
                                        
                                        for idx, phase in enumerate(sorted(crosstab_phase.columns)):
                                            fig.add_trace(go.Bar(
                                                name=str(phase),
                                                x=crosstab_phase.index,
                                                y=crosstab_phase[phase],
                                                marker_color=colors[idx % len(colors)],
                                                text=[f"{v:.1f}%" for v in crosstab_phase[phase]],
                                                textposition='auto'
                                            ))
                                        
                                        fig.update_layout(
                                            barmode='group',
                                            title='å„é¡åˆ¥åœ¨ä¸åŒéšæ®µçš„åˆ†ä½ˆæ¯”ä¾‹',
                                            xaxis_title='é¡åˆ¥',
                                            yaxis_title='æ¯”ä¾‹ (%)',
                                            template='plotly_white',
                                            height=400,
                                            xaxis_tickangle=-45,
                                            xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                        
                                        # å¡æ–¹æª¢å®š
                                        try:
                                            count_table = pd.crosstab(df_cat_phase['category'], df_cat_phase['phase'])
                                            chi2, p_val, dof, exp = chi2_contingency(count_table)
                                            
                                            significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*"
                                            
                                            st.markdown("**çµ±è¨ˆæª¢å®šçµæœï¼ˆå¡æ–¹æª¢å®šï¼‰ï¼š**")
                                            st.write(f"- p-value = {p_val:.4f} {significance}")
                                            
                                            if p_val < 0.05:
                                                st.success("âœ… ä¸åŒéšæ®µçš„åˆ†ä½ˆæœ‰é¡¯è‘—å·®ç•°")
                                            else:
                                                st.info("â„¹ï¸ ä¸åŒéšæ®µçš„åˆ†ä½ˆç„¡é¡¯è‘—å·®ç•°")
                                        except Exception as e:
                                            st.warning(f"ç„¡æ³•é€²è¡Œçµ±è¨ˆæª¢å®šï¼š{str(e)}")
                        
                        # === åœ–è¡¨èªªæ•…äº‹ ===
                        st.markdown("---")
                        st.markdown("#### ğŸ’¡ åˆ†ææ´å¯Ÿ")
                        
                        insights = []
                        
                        # æ ¹æ“šçµ±è¨ˆçµæœç”Ÿæˆæ´å¯Ÿ
                        if 'é¡¯è‘—é¸é …' in rec_info['çµ±è¨ˆçµæœ']:
                            sig_count = rec_info['çµ±è¨ˆçµæœ'].get('é¡¯è‘—é¸é …æ•¸', 0)
                            insights.append(f"ğŸ“Œ æœ¬é¡Œæœ‰ {sig_count} å€‹é¸é …åœ¨å…¬å¸æ–¹èˆ‡æŠ•è³‡æ–¹ä¹‹é–“å‘ˆç¾é¡¯è‘—å·®ç•°ï¼Œé¡¯ç¤ºå…©è€…å°æ­¤è­°é¡Œçš„çœ‹æ³•æˆ–å¯¦å‹™åšæ³•å­˜åœ¨æ˜é¡¯ä¸åŒã€‚")
                        
                        if 'p' in rec_info['çµ±è¨ˆçµæœ']:
                            p_val = rec_info['çµ±è¨ˆçµæœ']['p']
                            if p_val < 0.001:
                                insights.append("ğŸ“Œ çµ±è¨ˆæª¢å®šé¡¯ç¤ºæ¥µåº¦é¡¯è‘—å·®ç•° (p < 0.001)ï¼Œå»ºè­°åœ¨å ±å‘Šä¸­é‡é»æ¢è¨é€ æˆå·®ç•°çš„åŸå› ã€‚")
                            elif p_val < 0.01:
                                insights.append("ğŸ“Œ çµ±è¨ˆæª¢å®šé¡¯ç¤ºé«˜åº¦é¡¯è‘—å·®ç•° (p < 0.01)ï¼Œå€¼å¾—é€²ä¸€æ­¥åˆ†æä¸åŒç¾¤é«”çš„ç‰¹æ€§ã€‚")
                            elif p_val < 0.05:
                                insights.append("ğŸ“Œ çµ±è¨ˆæª¢å®šé¡¯ç¤ºé¡¯è‘—å·®ç•° (p < 0.05)ï¼Œå¯åœ¨å ±å‘Šä¸­æåŠæ­¤ç™¼ç¾ã€‚")
                        
                        if rec_info['ç¼ºå¤±ç‡'] == "0.0%":
                            insights.append("ğŸ“Œ æœ¬é¡Œè³‡æ–™å®Œæ•´åº¦æ¥µé«˜ï¼ˆç„¡ç¼ºå¤±å€¼ï¼‰ï¼Œåˆ†æçµæœå¯ä¿¡åº¦é«˜ã€‚")
                        
                        if insights:
                            for insight in insights:
                                st.write(insight)
                        else:
                            st.info("â„¹ï¸ æœ¬é¡Œæœªç™¼ç¾é¡¯è‘—çš„çµ±è¨ˆå·®ç•°ï¼Œä½†ä»å¯ä½œç‚ºæè¿°æ€§çµ±è¨ˆä½¿ç”¨ã€‚")
        else:
            st.info("ğŸ’¡ ç›®å‰æ²’æœ‰é«˜å„ªå…ˆé †åºï¼ˆâ‰¥ 2ï¼‰çš„é¡Œç›®ï¼Œå»ºè­°é™ä½ç¯©é¸æ¨™æº–æˆ–æª¢æŸ¥è³‡æ–™å“è³ªã€‚")
    else:
        st.warning("æœªæ‰¾åˆ°å…·æœ‰é¡¯è‘—å·®ç•°çš„é¡Œç›®")
    
    # === æ–°å¢ï¼šå°ˆæ¥­å ±å‘Šç”Ÿæˆ ===
    if recommendations:
        st.markdown("---")
        st.markdown("### ğŸ“„ å°ˆæ¥­åˆ†æå ±å‘Šç”Ÿæˆ")
        st.info("âœ¨ ç‚ºåœ‹ç™¼åŸºé‡‘é‡èº«æ‰“é€ çš„å°ˆæ¥­åˆ†æå ±å‘Šï¼Œæ¡ç”¨ã€ŒåŸ·è¡Œæ‘˜è¦ â†’ æ–¹æ³•è«– â†’ ä¸»è¦ç™¼ç¾ â†’ çµè«–èˆ‡å»ºè­°ã€çµæ§‹")
        
        if st.button("ğŸ“Š ç”Ÿæˆå®Œæ•´åˆ†æå ±å‘Š", type="primary"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆå°ˆæ¥­å ±å‘Š..."):
                # ç”Ÿæˆå ±å‘Šå…§å®¹
                report = generate_professional_report(df_to_analyze, recommendations, cols_to_analyze, analysis_mode)
                
                # é¡¯ç¤ºå ±å‘Š
                st.markdown("---")
                st.markdown(report, unsafe_allow_html=True)
                
                # æä¾›ä¸‹è¼‰é¸é …
                st.markdown("---")
                st.download_button(
                    label="ğŸ’¾ ä¸‹è¼‰å ±å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰",
                    data=report,
                    file_name=f"æœªä¸Šå¸‚æ«ƒå…¬å¸æ²»ç†å•å·åˆ†æå ±å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown"
                )

# --- é¡Œç›®é¡¯ç¤ºå€ ---
st.markdown("---")
st.markdown("### ğŸ“ é¡Œç›®åˆ†æèˆ‡è¦–è¦ºåŒ–")

expand_all = st.checkbox("ä¸€éµå±•é–‹/æ”¶åˆæ‰€æœ‰é¡Œç›®", value=False, key="expand_all_toggle")
st.markdown("---")

# é€é¡Œé¡¯ç¤º
for i, col_name in enumerate(cols_to_analyze):
    if col_name not in df_to_analyze.columns:
        continue
        
    col_data = df_to_analyze[col_name].dropna()
    if col_data.empty:
        continue
        
    with st.expander(f"é¡Œç›® {i+1}ï¼š{col_name}", expanded=expand_all):
        # é¡¯ç¤ºæ¨£æœ¬æ•¸
        st.caption(f"æœ‰æ•ˆæ¨£æœ¬æ•¸ï¼š{len(col_data)}")
        
        # åˆ¤æ–·é¡Œå‹
        is_multiselect = False
        if col_data.dtype == 'object':
            non_empty_data = col_data[col_data.astype(str) != '']
            if not non_empty_data.empty and non_empty_data.str.contains('\n').any():
                is_multiselect = True
        
        if is_multiselect:
            # è¤‡é¸é¡Œ
            st.markdown("##### ğŸ“Š è¤‡é¸é¡Œé¸é …æ¬¡æ•¸åˆ†ä½ˆ")
            exploded = col_data.astype(str).str.split('\n').explode().str.strip()
            exploded = exploded[(exploded != '') & (exploded != 'nan') & exploded.notna()]
            
            if not exploded.empty:
                total_counts = exploded.value_counts().reset_index()
                total_counts.columns = ['é¸é …', 'æ¬¡æ•¸']
                st.dataframe(total_counts, use_container_width=True)
                
                # è¦–è¦ºåŒ–ï¼šå¦‚æœæœ‰éšæ®µæ¬„ä½å‰‡æŒ‰éšæ®µåˆ†è‰²å †ç–Š
                if PHASE_COLUMN_NAME in df_to_analyze.columns and df_to_analyze[PHASE_COLUMN_NAME].notna().any() and df_to_analyze[PHASE_COLUMN_NAME].nunique() > 1:
                    st.markdown("##### ğŸ“ˆ å„éšæ®µåˆ†ä½ˆï¼ˆå †ç–Šé•·æ¢åœ–ï¼‰")
                    exploded_df = exploded.to_frame(name='option')
                    exploded_df['phase'] = df_to_analyze.loc[exploded_df.index, PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»éšæ®µ')
                    pivot = exploded_df.groupby(['option', 'phase']).size().unstack(fill_value=0)
                    
                    # æ™ºæ…§æ’åº x è»¸
                    sorted_index = smart_sort_categories(pivot.index)
                    pivot = pivot.reindex(sorted_index)
                    
                    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
                    fig = go.Figure()
                    for j, phase in enumerate(pivot.columns):
                        fig.add_trace(go.Bar(
                            x=pivot.index,
                            y=pivot[phase],
                            name=str(phase),
                            marker_color=colors[j % len(colors)]
                        ))
                    fig.update_layout(
                        barmode='stack', 
                        xaxis_tickangle=-45, 
                        template="plotly_white", 
                        height=500,
                        xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                    )
                    st.plotly_chart(fig, use_container_width=True, key=f"multi_{i}_{col_name[:20]}")
                else:
                    st.markdown("##### ğŸ“ˆ é•·æ¢åœ–")
                    # æ™ºæ…§æ’åº x è»¸
                    sorted_index = smart_sort_categories(total_counts['é¸é …'])
                    total_counts_sorted = total_counts.set_index('é¸é …').reindex(sorted_index).reset_index()
                    
                    fig = go.Figure(data=[go.Bar(x=total_counts_sorted['é¸é …'], y=total_counts_sorted['æ¬¡æ•¸'])])
                    fig.update_layout(
                        xaxis_tickangle=-45, 
                        template="plotly_white", 
                        height=500,
                        xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                    )
                    st.plotly_chart(fig, use_container_width=True, key=f"multi_{i}_{col_name[:20]}")
            
            # çµ±è¨ˆåˆ†æ - è¤‡é¸é¡Œ
            perform_comprehensive_statistical_analysis(df_to_analyze, col_data, col_name, is_numeric=False, is_multiselect=True)
        else:
            # å–®é¸æˆ–æ•¸å€¼é¡Œ
            is_numeric = pd.api.types.is_numeric_dtype(col_data)
            if not is_numeric:
                numeric_version = pd.to_numeric(col_data, errors='coerce')
                if (numeric_version.notna().sum() / len(col_data) > 0.7):
                    is_numeric = True
                    col_data = numeric_version.dropna()
            
            if is_numeric:
                # æ•¸å€¼é¡Œ
                st.markdown("##### ğŸ“Š æ•¸å€¼çµ±è¨ˆæ‘˜è¦")
                st.dataframe(col_data.describe().to_frame().T.style.format("{:,.2f}"), use_container_width=True)
                
                # ç›’ç‹€åœ–ï¼šå¦‚æœæœ‰éšæ®µæ¬„ä½å‰‡æŒ‰éšæ®µåˆ†çµ„é¡¯ç¤º
                if PHASE_COLUMN_NAME in df_to_analyze.columns and df_to_analyze[PHASE_COLUMN_NAME].notna().any() and df_to_analyze[PHASE_COLUMN_NAME].nunique() > 1:
                    st.markdown("##### ğŸ“¦ ç›’ç‹€åœ–ï¼ˆå„éšæ®µæ¯”è¼ƒï¼‰")
                    df_numeric = col_data.to_frame(name='value')
                    df_numeric['phase'] = df_to_analyze.loc[df_numeric.index, PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»éšæ®µ')
                    
                    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
                    fig = go.Figure()
                    for j, phase in enumerate(sorted(df_numeric['phase'].unique())):
                        phase_data = df_numeric[df_numeric['phase'] == phase]['value']
                        fig.add_trace(go.Box(
                            y=phase_data,
                            name=str(phase),
                            marker_color=colors[j % len(colors)]
                        ))
                    fig.update_layout(template="plotly_white", height=400, showlegend=True)
                    st.plotly_chart(fig, use_container_width=True, key=f"num_{i}_{col_name[:20]}")
                else:
                    st.markdown("##### ğŸ“¦ ç›’ç‹€åœ–")
                    fig = go.Figure(data=[go.Box(y=col_data, name=col_name[:50])])
                    fig.update_layout(template="plotly_white", height=400)
                    st.plotly_chart(fig, use_container_width=True, key=f"num_{i}_{col_name[:20]}")
                
                # çµ±è¨ˆåˆ†æ - æ•¸å€¼é¡Œ
                perform_comprehensive_statistical_analysis(df_to_analyze, col_data, col_name, is_numeric=True, is_multiselect=False)
            else:
                # é¡åˆ¥é¡Œ
                st.markdown("##### ğŸ“Š é¡åˆ¥æ¬¡æ•¸åˆ†ä½ˆ")
                s = col_data.astype(str)
                s = s[~s.str.lower().str.contains('nan', na=False)]
                
                if not s.empty:
                    total = s.value_counts().reset_index()
                    total.columns = ['é¸é …', 'æ¬¡æ•¸']
                    st.dataframe(total, use_container_width=True)
                    
                    # è¦–è¦ºåŒ–ï¼šå¦‚æœæœ‰éšæ®µæ¬„ä½å‰‡æŒ‰éšæ®µåˆ†è‰²å †ç–Š
                    if PHASE_COLUMN_NAME in df_to_analyze.columns and df_to_analyze[PHASE_COLUMN_NAME].notna().any() and df_to_analyze[PHASE_COLUMN_NAME].nunique() > 1:
                        st.markdown("##### ğŸ“ˆ å„éšæ®µåˆ†ä½ˆï¼ˆå †ç–Šé•·æ¢åœ–ï¼‰")
                        df_pair = s.to_frame(name='ans')
                        df_pair['phase'] = df_to_analyze.loc[df_pair.index, PHASE_COLUMN_NAME].fillna('æœªæ¨™è¨»éšæ®µ')
                        pivot = df_pair.groupby(['ans', 'phase']).size().unstack(fill_value=0)
                        
                        # æ™ºæ…§æ’åº x è»¸
                        sorted_index = smart_sort_categories(pivot.index)
                        pivot = pivot.reindex(sorted_index)
                        
                        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
                        fig = go.Figure()
                        for j, phase in enumerate(pivot.columns):
                            fig.add_trace(go.Bar(
                                x=pivot.index,
                                y=pivot[phase],
                                name=str(phase),
                                marker_color=colors[j % len(colors)]
                            ))
                        fig.update_layout(
                            barmode='stack', 
                            xaxis_tickangle=-45, 
                            template="plotly_white", 
                            height=500,
                            xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                        )
                        st.plotly_chart(fig, use_container_width=True, key=f"cat_{i}_{col_name[:20]}")
                    else:
                        st.markdown("##### ğŸ“ˆ é•·æ¢åœ–")
                        # æ™ºæ…§æ’åº x è»¸
                        sorted_index = smart_sort_categories(total['é¸é …'])
                        total_sorted = total.set_index('é¸é …').reindex(sorted_index).reset_index()
                        
                        fig = go.Figure(data=[go.Bar(x=total_sorted['é¸é …'], y=total_sorted['æ¬¡æ•¸'])])
                        fig.update_layout(
                            xaxis_tickangle=-45, 
                            template="plotly_white", 
                            height=500,
                            xaxis={'categoryorder': 'array', 'categoryarray': sorted_index}
                        )
                        st.plotly_chart(fig, use_container_width=True, key=f"cat_{i}_{col_name[:20]}")
                
                # çµ±è¨ˆåˆ†æ - é¡åˆ¥é¡Œ
                perform_comprehensive_statistical_analysis(df_to_analyze, col_data, col_name, is_numeric=False, is_multiselect=False)
